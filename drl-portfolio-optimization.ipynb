{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Portfolio Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "This workbook is the culmination of three separate half credit independent studies by the author [Daniel Fudge](https://www.linkedin.com/in/daniel-fudge) with [Professor Yelena Larkin](https://www.linkedin.com/in/yelena-larkin-6b7b361b/) \n",
    "as part of a concurrent Master of Business Administration (MBA) and a [Diploma in Financial Engineering](https://schulich.yorku.ca/programs/fnen/) from the [Schulich School of Business](https://schulich.yorku.ca/).  I wanted to thank Schulich and especially Professor Larkin for giving me the freedom to explorer the intersection of Machine Learning and Finance.   \n",
    "\n",
    "I'd like to also mention the excellent training I recieved from [A Cloud Guru](https://acloud.guru/learn/aws-certified-machine-learning-specialty) to prepare for this project.  \n",
    "\n",
    "This notebook takes much of the design and low level code from the AWS sample portfolio management [Notebook](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_portfolio_management_coach_customEnv), which inturn is based on Jiang, Zhengyao, Dixing Xu, and Jinjun Liang. \"A deep reinforcement learning framework for the financial portfolio management problem.\" arXiv preprint arXiv:1706.10059 (2017).    \n",
    "\n",
    "As detailed below, this notebook relies on the [RL Coach](https://github.com/NervanaSystems/coach#batch-reinforcement-learning) from Intel AI Labs, Apache [MXNet](https://mxnet.apache.org/) and OpenAI [Gym](https://gym.openai.com/).  All of which are amazing projects that I highly recommend investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Now that we have the signals `signals.pkl` and prices `stock-data-clean.pkl` processed we can build the Reinforcement Learning algorithm.  The signals will be used to define the state of the market environment, called `observations` in the Gym documentation and `state` in other locations.  The prices will be used to generate the `rewards`.   \n",
    "\n",
    "For a refresher on DRL I recommend looking through [report2](docs/report2.pdf) in this repo.  The basic RL feed back loop is shown below.    \n",
    "\n",
    "![RL](docs/rl.png \"Reinforcement Learning Loop\")\n",
    "\n",
    "Reinforcement Learning feedback loop.    \n",
    "Image source: https://i.stack.imgur.com/eoeSq.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "The `action` space is simply vector containing the weights of the stocks in the portfolio.  Inside the environment these weights are limited to (0, 1) and an addition weight is added for the amount of cash in the portfolio.  The cash weight is simply one minus the sum of the other weights and also limited to (0, 1).  The last step is to normalize these weight so the sum is equal to 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State (or Observation)\n",
    "The `state` is simply the signals we compiled in the previous data preparation [notebook](docs/data-preparation-no-memory.ipynb) including a time embbeding called the `window_length` within the code.  Instead of using the LSTM discussed previously, we are using a short Convolutional Neural Net (CNN) to capture some historical information from the data within the `agent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "For the agent we are using the [RL Coach](https://github.com/NervanaSystems/coach#batch-reinforcement-learning) from Intel AI Labs that is integrated into AWS [Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/reinforcement-learning.html#sagemaker-rl).  As you can see below there are a large number of RL algorithms to choose from.  In  [report2](docs/report2.pdf) we discussed serval of these and focused on the Deep Deterministic Policy Gradient (DDPG) Actor-Critic method however for this test we are trying the Proximal Policy Optimization (PPO) algorithm that is explained nicely by Jonathan Hui [here](https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12) and the original paper by Schulman et al. [here](https://arxiv.org/pdf/1707.06347.pdf).    \n",
    "\n",
    "Remember that the goal of the agent is to learn a policy that maximizes the sum of discounted rewards, which in our case will result in the maximization of the portfolio value.\n",
    "\n",
    "![Coach](docs/rl-coach.png)\n",
    "RL Coach Algorithms\n",
    "Imafe Source:  https://github.com/NervanaSystems/coach#batch-reinforcement-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweaks to Agent\n",
    "If you are playing with the number of signals you may have to change the input to the Agent.  Unfortunately one of the disadvantages of using pre-built frameworks is the extra overhead and the difficulity finding the little details you need to tweak.   \n",
    "\n",
    "Here we have to define input, which for us is a 2D CNN or Conv2D as a RL Coach [input embedder](https://nervanasystems.github.io/coach/design/network.html) but it is actually passed to the Conv2D within Apache [MXNet].   The RL Coach [code](https://github.com/NervanaSystems/coach/blob/master/rl_coach/architectures/layers.py) that passes the arguments to the MXNet Conv2D [code](https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.nn.Conv2D.html) only accepts the 3 arguments and the MXNet infers the rest.  This lack of control makes setting up the problem a little tricky.  Also for time series data the MXNet [Conv1D](https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.nn.Conv1D.html) would be more appropriate but RL coach doesn't give us the flexibility.    \n",
    "\n",
    "The interface to RL Coach is defined in `preset-portfolio-management-clippedppo.py` shown below.  The line you may want to tweak is under \"Agent\" and contains the Conv2D argument.  This needs to coorespond to the observation space definition and how the observation is pulled from the signals in the `portfolio_env.py` file shown below as well.  Both of these are in the `src` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.agents.clipped_ppo_agent\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClippedPPOAgentParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.architectures.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Conv2d\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VisualizationParameters, PresetValidationParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MiddlewareScheme, DistributedCoachSynchronizationType\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.core_types\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, RunPhase\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.environments.gym_environment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GymVectorEnvironment, ObservationSpaceType\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.exploration_policies.e_greedy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m EGreedyParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.basic_rl_graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BasicRLGraphManager\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ScheduleParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.schedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LinearSchedule\r\n",
      "\r\n",
      "\u001b[37m####################\u001b[39;49;00m\r\n",
      "\u001b[37m# Graph Scheduling #\u001b[39;49;00m\r\n",
      "\u001b[37m####################\u001b[39;49;00m\r\n",
      "\r\n",
      "schedule_params = ScheduleParameters()\r\n",
      "schedule_params.improve_steps = TrainingSteps(\u001b[34m20000\u001b[39;49;00m)\r\n",
      "schedule_params.steps_between_evaluation_periods = EnvironmentSteps(\u001b[34m2048\u001b[39;49;00m)\r\n",
      "schedule_params.evaluation_steps = EnvironmentEpisodes(\u001b[34m5\u001b[39;49;00m)\r\n",
      "schedule_params.heatup_steps = EnvironmentSteps(\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m#########\u001b[39;49;00m\r\n",
      "\u001b[37m# Agent #\u001b[39;49;00m\r\n",
      "\u001b[37m#########\u001b[39;49;00m\r\n",
      "\r\n",
      "agent_params = ClippedPPOAgentParameters()\r\n",
      "\r\n",
      "\u001b[37m# Input Embedder with no CNN\u001b[39;49;00m\r\n",
      "\u001b[37m#agent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = [Dense(71)]\u001b[39;49;00m\r\n",
      "\u001b[37m#agent_params.network_wrappers['main'].input_embedders_parameters['observation'].activation_function = 'tanh'\u001b[39;49;00m\r\n",
      "\u001b[37m#agent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense(128)]\u001b[39;49;00m\r\n",
      "\u001b[37m#agent_params.network_wrappers['main'].middleware_parameters.activation_function = 'tanh'\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Input Embedder used in sample notebook\u001b[39;49;00m\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].input_embedders_parameters[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].scheme = [Conv2d(\u001b[34m32\u001b[39;49;00m, [\u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m], \u001b[34m1\u001b[39;49;00m)]\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].middleware_parameters.scheme = MiddlewareScheme.Empty\r\n",
      "\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].learning_rate = \u001b[34m0.0001\u001b[39;49;00m\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].batch_size = \u001b[34m64\u001b[39;49;00m\r\n",
      "agent_params.algorithm.clipping_decay_schedule = LinearSchedule(\u001b[34m1.0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m, \u001b[34m150000\u001b[39;49;00m)\r\n",
      "agent_params.algorithm.discount = \u001b[34m0.99\u001b[39;49;00m\r\n",
      "agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(\u001b[34m2048\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m# Distributed Coach synchronization type.\u001b[39;49;00m\r\n",
      "agent_params.algorithm.distributed_coach_synchronization_type = DistributedCoachSynchronizationType.SYNC\r\n",
      "\r\n",
      "agent_params.exploration = EGreedyParameters()\r\n",
      "agent_params.exploration.epsilon_schedule = LinearSchedule(\u001b[34m1.0\u001b[39;49;00m, \u001b[34m0.01\u001b[39;49;00m, \u001b[34m10000\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m###############\u001b[39;49;00m\r\n",
      "\u001b[37m# Environment #\u001b[39;49;00m\r\n",
      "\u001b[37m###############\u001b[39;49;00m\r\n",
      "\r\n",
      "env_params = GymVectorEnvironment(level=\u001b[33m'\u001b[39;49;00m\u001b[33mportfolio_env:PortfolioEnv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "env_params.\u001b[31m__dict__\u001b[39;49;00m[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation_space_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = ObservationSpaceType.Tensor\r\n",
      "\r\n",
      "\u001b[37m########\u001b[39;49;00m\r\n",
      "\u001b[37m# Test #\u001b[39;49;00m\r\n",
      "\u001b[37m########\u001b[39;49;00m\r\n",
      "\r\n",
      "preset_validation_params = PresetValidationParameters()\r\n",
      "preset_validation_params.test = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "\r\n",
      "graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\r\n",
      "                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\r\n",
      "                                    preset_validation_params=preset_validation_params)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/preset-portfolio-management-clippedppo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "This leaves us with the environment to generate.  Here we build a custom finacial market environment (or simulator) based on the signals and prices we compiled previously on top of the OpenAI [Gym](https://gym.openai.com/), which is also integrated into AWS [Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-rl-environments.html#sagemaker-rl-environments-gym).  This takes care of the low level integration allows us to focus on the features of the environment specific to portfolio optimization.   \n",
    "\n",
    "During each training epoch, the trainer randomly selects a date to start.  It then steps through each day in the epoch and following these high level operations.\n",
    "1. Initialze the state from the signals with the randomly selected start date and sets the portfolio to $1 of cash.\n",
    "1. Pass the state to the agent who generates an action, which is a new set of desired portfolio weights.\n",
    "1. Pass the new weigths (action) to the environment who calculates: \n",
    "  * The new portfolio value based on changes in the prices, weights and transaction costs.   \n",
    "  * The reward based on the previous weights and the change in prices.\n",
    "  * The new state, which is simply pulled from the signals dataset.\n",
    "1. Pass the new reward and state to the agent, who must then learn to make a better action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Portfolio Environment\n",
    "The code below implements the custom financial market environment that the agent must trade in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\" Modified from https://github.com/awslabs/amazon-sagemaker-examples \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym.spaces\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\r\n",
      "EPS = \u001b[34m1e-8\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mPortfolioEnv\u001b[39;49;00m(gym.Env):\r\n",
      "    \u001b[33m\"\"\" This class creates the financial market environment that the Agent interact with.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    It extends the OpenAI Gym environment https://gym.openai.com/.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    More information of how it is integrated into AWS is found here\u001b[39;49;00m\r\n",
      "\u001b[33m    https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-rl-environments.html#sagemaker-rl-environments-gym\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    The observations include a history of the signals with the given `window_length` ending at the current date.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        steps (int):  Steps or days in an episode.\u001b[39;49;00m\r\n",
      "\u001b[33m        trading_cost (float):  Cost of trade as a fraction.\u001b[39;49;00m\r\n",
      "\u001b[33m        window_length (int):  How many past observations to return.\u001b[39;49;00m\r\n",
      "\u001b[33m        start_date_index (int):  The date index in the signals and price arrays.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Attributes:\u001b[39;49;00m\r\n",
      "\u001b[33m        action_space (gym.spaces.Box):  [n_tickers]  The portfolio weighting not including cash.\u001b[39;49;00m\r\n",
      "\u001b[33m        dates (np.array of np.datetime64):  [n_days] Dates for the signals and price history arrays.\u001b[39;49;00m\r\n",
      "\u001b[33m        info_list (list):  List of info dictionaries for each step.\u001b[39;49;00m\r\n",
      "\u001b[33m        n_signals (int):  Number of signals in each observation.\u001b[39;49;00m\r\n",
      "\u001b[33m        n_tickers (int):  Number of tickers in the price history.\u001b[39;49;00m\r\n",
      "\u001b[33m        observation_space (gym.spaces.Box)  [self.n_signals, window_length]  The signals with a window_length history.\u001b[39;49;00m\r\n",
      "\u001b[33m        portfolio_value (float):  The portfolio value, starting with $1 in cash.\u001b[39;49;00m\r\n",
      "\u001b[33m        gain (np.array):  [n_days, n_tickers, gain] The relative price vector; today's / yesterday's price.\u001b[39;49;00m\r\n",
      "\u001b[33m        signals (np.array):  [n_signals, n_days, 1]  Signals that define the observable environment.\u001b[39;49;00m\r\n",
      "\u001b[33m        start_date_index (int):  The date index in the signals and price arrays.\u001b[39;49;00m\r\n",
      "\u001b[33m        step_number (int):  The step number of the episode.\u001b[39;49;00m\r\n",
      "\u001b[33m        steps (int):  Steps or days in an episode.\u001b[39;49;00m\r\n",
      "\u001b[33m        test (bool):  Indicates if this is a test or training session.\u001b[39;49;00m\r\n",
      "\u001b[33m        test_length (int):  Trading days reserved for testing.\u001b[39;49;00m\r\n",
      "\u001b[33m        tickers (list of str):  The stock tickers.\u001b[39;49;00m\r\n",
      "\u001b[33m        trading_cost (float):  Cost of trade as a fraction.\u001b[39;49;00m\r\n",
      "\u001b[33m        window_length (int):  How many past observations to return.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, steps=\u001b[34m506\u001b[39;49;00m, trading_cost=\u001b[34m0.0025\u001b[39;49;00m, window_length=\u001b[34m5\u001b[39;49;00m, start_date_index=\u001b[34m4\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"An environment for financial portfolio management.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Initialize some local parameters\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.csv = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/output/data/portfolio-management.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.info_list = \u001b[36mlist\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.portfolio_value = \u001b[34m1.0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.step_number = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.steps = steps\r\n",
      "        \u001b[36mself\u001b[39;49;00m.test_length = \u001b[34m506\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.trading_cost = trading_cost\r\n",
      "\r\n",
      "        \u001b[37m# Save some arguments as attributes\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.window_length = window_length\r\n",
      "\r\n",
      "        \u001b[37m# Determine if this is a test or training session and limit the start_date_index accordingly\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.test = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(os.path.dirname(\u001b[31m__file__\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33msession-type.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "            tmp = f.read().lower()\r\n",
      "            \u001b[34mif\u001b[39;49;00m tmp.startswith(\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "                \u001b[36mself\u001b[39;49;00m.test = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "            \u001b[34melif\u001b[39;49;00m tmp.startswith(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "                \u001b[36mself\u001b[39;49;00m.test = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "            \u001b[34melse\u001b[39;49;00m:\r\n",
      "                \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSession type not defined!!!\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[37m# Read the stock data and convert to the relative price vector (gain)\u001b[39;49;00m\r\n",
      "        \u001b[37m#   Note the raw prices have an extra day vs the signals to calculate gain\u001b[39;49;00m\r\n",
      "        raw_prices = pd.read_csv(os.path.join(os.path.dirname(\u001b[31m__file__\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mprices.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),index_col=\u001b[34m0\u001b[39;49;00m, parse_dates=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.tickers = raw_prices.columns.tolist()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.gain = np.hstack((np.ones((raw_prices.shape[\u001b[34m0\u001b[39;49;00m]-\u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)), raw_prices.values[\u001b[34m1\u001b[39;49;00m:] / raw_prices.values[:-\u001b[34m1\u001b[39;49;00m]))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dates = raw_prices.index.values[\u001b[34m1\u001b[39;49;00m:]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.n_dates = \u001b[36mself\u001b[39;49;00m.dates.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.n_tickers = \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.tickers)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.weights = np.insert(np.zeros(\u001b[36mself\u001b[39;49;00m.n_tickers), \u001b[34m0\u001b[39;49;00m, \u001b[34m1.0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.test:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.start_date_index = start_date_index\r\n",
      "        \u001b[34melse\u001b[39;49;00m:            \r\n",
      "            \u001b[37m# The start index must >= the window length to avoid a negative index or data leakage\u001b[39;49;00m\r\n",
      "            \u001b[36mself\u001b[39;49;00m.start_date_index = \u001b[36mmax\u001b[39;49;00m(start_date_index, \u001b[36mself\u001b[39;49;00m.window_length - \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "            \u001b[37m# The start index <= n_dates - test length - the steps in the episode to avoid reading test data\u001b[39;49;00m\r\n",
      "            \u001b[36mself\u001b[39;49;00m.start_date_index = \u001b[36mmin\u001b[39;49;00m(start_date_index, \u001b[36mself\u001b[39;49;00m.n_dates - \u001b[36mself\u001b[39;49;00m.test_length - \u001b[36mself\u001b[39;49;00m.steps)\r\n",
      "\r\n",
      "        \u001b[37m# Read the signals\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.signals = pd.read_csv(os.path.join(os.path.dirname(\u001b[31m__file__\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33msignals.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                                   index_col=\u001b[34m0\u001b[39;49;00m, parse_dates=\u001b[36mTrue\u001b[39;49;00m).T.values[:, :, np.newaxis]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.n_signals = \u001b[36mself\u001b[39;49;00m.signals.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "\r\n",
      "        \u001b[37m# Define the action space as the portfolio weights where wn are [0, 1] for each asset not including cash\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.action_space = gym.spaces.Box(low=\u001b[34m0\u001b[39;49;00m, high=\u001b[34m1\u001b[39;49;00m, shape=(\u001b[36mself\u001b[39;49;00m.n_tickers,), dtype=np.float32)\r\n",
      "\r\n",
      "        \u001b[37m# Define the observation space, which are the signals\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.observation_space = gym.spaces.Box(low=-\u001b[34m1\u001b[39;49;00m, high=\u001b[34m1\u001b[39;49;00m, shape=(\u001b[36mself\u001b[39;49;00m.n_signals, \u001b[36mself\u001b[39;49;00m.window_length, \u001b[34m1\u001b[39;49;00m), dtype=np.float32)\r\n",
      "\r\n",
      "    \u001b[37m# -----------------------------------------------------------------------------------\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mstep\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, action):\r\n",
      "        \u001b[33m\"\"\"Step the environment.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        See https://gym.openai.com/docs/#observations for detailed description of return values.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Args:\u001b[39;49;00m\r\n",
      "\u001b[33m            action (np.array):  The desired portfolio weights [w0...].\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m            np.array:  [n_signals, window_length, 1] The observation of the environment (state)\u001b[39;49;00m\r\n",
      "\u001b[33m            float:  The reward received from the previous action.\u001b[39;49;00m\r\n",
      "\u001b[33m            bool:  Indicates if the simulation is complete.\u001b[39;49;00m\r\n",
      "\u001b[33m            dict:  Debugging information.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.step_number += \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Force the new weights (w1) to (0.0, 1.0) and sum weights = 1, note 1st weight is cash.\u001b[39;49;00m\r\n",
      "        w1 = np.clip(action, a_min=\u001b[34m0\u001b[39;49;00m, a_max=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        w1 = np.insert(w1, \u001b[34m0\u001b[39;49;00m, np.clip(\u001b[34m1\u001b[39;49;00m - w1.sum(), a_min=\u001b[34m0\u001b[39;49;00m, a_max=\u001b[34m1\u001b[39;49;00m))\r\n",
      "        w1 = w1 / w1.sum()\r\n",
      "\r\n",
      "        \u001b[37m# Calculate the reward; Numbered equations are from https://arxiv.org/abs/1706.10059\u001b[39;49;00m\r\n",
      "        t = \u001b[36mself\u001b[39;49;00m.start_date_index + \u001b[36mself\u001b[39;49;00m.step_number\r\n",
      "        y1 = \u001b[36mself\u001b[39;49;00m.gain[t]\r\n",
      "        w0 = \u001b[36mself\u001b[39;49;00m.weights\r\n",
      "        p0 = \u001b[36mself\u001b[39;49;00m.portfolio_value\r\n",
      "        dw1 = (y1 * w0) / (np.dot(y1, w0) + EPS)            \u001b[37m# (eq7) weights evolve into\u001b[39;49;00m\r\n",
      "        mu1 = \u001b[36mself\u001b[39;49;00m.trading_cost * (np.abs(dw1 - w1)).sum()  \u001b[37m# (eq16) cost to change portfolio\u001b[39;49;00m\r\n",
      "        p1 = p0 * (\u001b[34m1\u001b[39;49;00m - mu1) * np.dot(y1, w1)                \u001b[37m# (eq11) final portfolio value\u001b[39;49;00m\r\n",
      "        p1 = np.clip(p1, \u001b[34m0\u001b[39;49;00m, np.inf)                         \u001b[37m# Limit portfolio to zero (busted)\u001b[39;49;00m\r\n",
      "        rho1 = p1 / p0 - \u001b[34m1\u001b[39;49;00m                                  \u001b[37m# rate of returns\u001b[39;49;00m\r\n",
      "        reward = np.log((p1 + EPS) / (p0 + EPS))            \u001b[37m# log rate of return\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Save weights and portfolio value for next iteration\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.weights = w1\r\n",
      "        \u001b[36mself\u001b[39;49;00m.portfolio_value = p1\r\n",
      "\r\n",
      "        \u001b[37m# Observe the new environment (state)\u001b[39;49;00m\r\n",
      "        t0 = t - \u001b[36mself\u001b[39;49;00m.window_length + \u001b[34m1\u001b[39;49;00m\r\n",
      "        observation = \u001b[36mself\u001b[39;49;00m.signals[:, t0:t+\u001b[34m1\u001b[39;49;00m, :]\r\n",
      "\r\n",
      "        \u001b[37m# Save some information for debugging and plotting at the end\u001b[39;49;00m\r\n",
      "        r = y1.mean()\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.step_number == \u001b[34m1\u001b[39;49;00m:\r\n",
      "            market_value = r\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            market_value = \u001b[36mself\u001b[39;49;00m.info_list[-\u001b[34m1\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mmarket_value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] * r \r\n",
      "        info = {\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: reward, \u001b[33m\"\u001b[39;49;00m\u001b[33mlog_return\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: reward, \u001b[33m\"\u001b[39;49;00m\u001b[33mportfolio_value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: p1, \u001b[33m\"\u001b[39;49;00m\u001b[33mreturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: r, \u001b[33m\"\u001b[39;49;00m\u001b[33mrate_of_return\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: rho1,\r\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mweights_mean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: w1.mean(), \u001b[33m\"\u001b[39;49;00m\u001b[33mweights_std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: w1.std(), \u001b[33m\"\u001b[39;49;00m\u001b[33mcost\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: mu1, \u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mself\u001b[39;49;00m.dates[t],\r\n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mself\u001b[39;49;00m.step_number, \u001b[33m\"\u001b[39;49;00m\u001b[33mmarket_value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: market_value}\r\n",
      "        \u001b[36mself\u001b[39;49;00m.info_list.append(info)\r\n",
      "\r\n",
      "        \u001b[37m# Check if finished and write to file\u001b[39;49;00m\r\n",
      "        done = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.step_number >= \u001b[36mself\u001b[39;49;00m.steps) \u001b[35mor\u001b[39;49;00m (p1 <= \u001b[34m0\u001b[39;49;00m):\r\n",
      "            done = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "            pd.DataFrame(\u001b[36mself\u001b[39;49;00m.info_list).sort_values(by=[\u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]).to_csv(\u001b[36mself\u001b[39;49;00m.csv)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m observation, reward, done, info\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mreset\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"Reset the environment to the initial state.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Ref:\u001b[39;49;00m\r\n",
      "\u001b[33m        https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-rl-environments.html#sagemaker-rl-environments-gym\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.info_list = \u001b[36mlist\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.weights = np.insert(np.zeros(\u001b[36mself\u001b[39;49;00m.n_tickers), \u001b[34m0\u001b[39;49;00m, \u001b[34m1.0\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.portfolio_value = \u001b[34m1.0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.step_number = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_date_index = \u001b[36mmax\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.start_date_index, \u001b[36mself\u001b[39;49;00m.window_length - \u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_date_index = \u001b[36mmin\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.start_date_index, \u001b[36mself\u001b[39;49;00m.n_dates - \u001b[36mself\u001b[39;49;00m.steps)\r\n",
      "\r\n",
      "        t = \u001b[36mself\u001b[39;49;00m.start_date_index + \u001b[36mself\u001b[39;49;00m.step_number\r\n",
      "        t0 = t - \u001b[36mself\u001b[39;49;00m.window_length + \u001b[34m1\u001b[39;49;00m\r\n",
      "        observation = \u001b[36mself\u001b[39;49;00m.signals[:, t0:t+\u001b[34m1\u001b[39;49;00m, :]\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m observation\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/portfolio_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Roles and permissions\n",
    "\n",
    "To get started, we'll import the Python libraries we need, set up the environment with a few prerequisites for permissions and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "from src.misc import get_execution_role, wait_for_s3_object\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "sys.path.append(\"common\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup S3 buckets\n",
    "\n",
    "Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint s3 path: s3://sagemaker-us-east-1-031118886020/checkpoints\n"
     ]
    }
   ],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "checkpoint_path = s3_output_path[:-1] + '/checkpoints'\n",
    "print(\"Checkpoint s3 path: {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "Get the execution IAM role that allows this notebook to connect to the training and evaluation instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IAM role arn: arn:aws:iam::031118886020:role/sagemaker\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the RL model using the Python SDK Script mode\n",
    "\n",
    "1. Specify the source directory where the environment, presets and training code is uploaded, `src`.\n",
    "2. Specify the `entry_point` as the training code,  \n",
    "3. Specify the choice of RL `toolkit` and framework. This automatically resolves to the ECR path for the RL Container. \n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name. \n",
    "5. Specify the hyperparameters for the RL agent algorithm. The `RLCOACH_PRESET` can be used to specify the RL agent algorithm you want to use. \n",
    "6. [Optional] Choose the metrics that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks. The metrics are defined using regular expression matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price reference\n",
    "Here are some price data for reference.  Obviously these values will change as you change the hyperparameters and model architecture.  Please check the pricing [list](https://aws.amazon.com/sagemaker/pricing/) for the latest pricing and instance types.\n",
    "- **ml.c4.xlarge:**   $423s =  7.1 min = 0.118 hr \\rightarrow \\$0.03 = 0.118 hr \\cdot 0.279 \\frac{\\$}{hr} $ \n",
    "- **ml.c4.2xlarge:**  $371s =  6.2 min = 0.103 hr \\rightarrow \\$0.06 = 0.103 hr \\cdot 0.557 \\frac{\\$}{hr} $ \n",
    "- **ml.c4.4xlarge:**  $380s =  6.3 min = 0.106 hr \\rightarrow \\$0.12 = 0.106 hr \\cdot 1.114 \\frac{\\$}{hr} $ \n",
    "- **ml.c4.8xlarge:**  $390s =  6.5 min = 0.108 hr \\rightarrow \\$0.24 = 0.108 hr \\cdot 2.227 \\frac{\\$}{hr} $ \n",
    "- **ml.m4.4xlarge:**  $414s =  6.9 min = 0.115 hr \\rightarrow \\$0.13 = 0.115 hr \\cdot 1.12 \\frac{\\$}{hr} $ \n",
    "- **ml.p3.2xlarge:**  $558s =  9.3 min = 0.155 hr \\rightarrow \\$0.66 = 0.155 hr \\cdot 4.284 \\frac{\\$}{hr}$\n",
    "- **ml.p2.xlarge:**   $713s = 11.9 min = 0.198 hr \\rightarrow \\$0.25 = 0.198 hr \\cdot 1.26 \\frac{\\$}{hr}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.c4.2xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to false to start training from scratch\n",
    "rerun = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-29 06:12:10 Starting - Starting the training job...\n",
      "2020-03-29 06:12:13 Starting - Launching requested ML instances.........\n",
      "2020-03-29 06:13:45 Starting - Preparing the instances for training......\n",
      "2020-03-29 06:15:05 Downloading - Downloading input data\n",
      "2020-03-29 06:15:05 Training - Downloading the training image...\n",
      "2020-03-29 06:15:25 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:27,601 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:27,604 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:27,617 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HPS': '{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"improve_steps\":100000,\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5,\"training_epochs\":10}', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"improve_steps\":100000,\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5,\"training_epochs\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"drl-portfolio-optimization-2020-03-29-06-12-09-827\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-29-06-12-09-827/source/sourcedir.tar.gz\",\"module_name\":\"train-coach\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-coach.py\"}', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_estimator\":\"RLEstimator\"}', 'SM_CHANNELS': '[]', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'train-coach', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-29-06-12-09-827/source/sourcedir.tar.gz', 'SM_HOSTS': '[\"algo-1\"]', 'SM_USER_ENTRY_POINT': 'train-coach.py', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_HP_RL.EVALUATION_STEPS:ENVIRONMENTEPISODES': '5', 'SM_HP_TRAINING_EPOCHS': '10', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HP_RL.AGENT_PARAMS.ALGORITHM.DISCOUNT': '0.9', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_HP_RLCOACH_PRESET': 'preset-portfolio-management-clippedppo', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_NUM_CPUS': '8', 'SM_USER_ARGS': '[\"--RLCOACH_PRESET\",\"preset-portfolio-management-clippedppo\",\"--improve_steps\",\"100000\",\"--rl.agent_params.algorithm.discount\",\"0.9\",\"--rl.evaluation_steps:EnvironmentEpisodes\",\"5\",\"--training_epochs\",\"10\"]', 'SM_HP_IMPROVE_STEPS': '100000', 'SM_LOG_LEVEL': '20', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_NUM_GPUS': '0'}\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:27,818 sagemaker-containers INFO     Module train-coach does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:27,818 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:27,818 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:27,818 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train-coach\n",
      "  Running setup.py bdist_wheel for train-coach: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train-coach: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n55m0ti3/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train-coach\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train-coach\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-coach-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:29,683 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-29 06:15:29,695 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-29-06-12-09-827/source/sourcedir.tar.gz\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator\": \"RLEstimator\"\n",
      "    },\n",
      "    \"num_gpus\": 0,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"num_cpus\": 8,\n",
      "    \"log_level\": 20,\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"input_data_config\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"hyperparameters\": {\n",
      "        \"rl.evaluation_steps:EnvironmentEpisodes\": 5,\n",
      "        \"rl.agent_params.algorithm.discount\": 0.9,\n",
      "        \"improve_steps\": 100000,\n",
      "        \"training_epochs\": 10,\n",
      "        \"RLCOACH_PRESET\": \"preset-portfolio-management-clippedppo\"\n",
      "    },\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"user_entry_point\": \"train-coach.py\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"job_name\": \"drl-portfolio-optimization-2020-03-29-06-12-09-827\",\n",
      "    \"module_name\": \"train-coach\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HPS={\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"improve_steps\":100000,\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5,\"training_epochs\":10}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-coach\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-29-06-12-09-827/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-coach.py\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_RL.EVALUATION_STEPS:ENVIRONMENTEPISODES=5\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HP_RL.AGENT_PARAMS.ALGORITHM.DISCOUNT=0.9\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_HP_RLCOACH_PRESET=preset-portfolio-management-clippedppo\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--RLCOACH_PRESET\",\"preset-portfolio-management-clippedppo\",\"--improve_steps\",\"100000\",\"--rl.agent_params.algorithm.discount\",\"0.9\",\"--rl.evaluation_steps:EnvironmentEpisodes\",\"5\",\"--training_epochs\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_HP_IMPROVE_STEPS=100000\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator\":\"RLEstimator\"}\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"improve_steps\":100000,\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5,\"training_epochs\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"drl-portfolio-optimization-2020-03-29-06-12-09-827\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-29-06-12-09-827/source/sourcedir.tar.gz\",\"module_name\":\"train-coach\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-coach.py\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train-coach --RLCOACH_PRESET preset-portfolio-management-clippedppo --improve_steps 100000 --rl.agent_params.algorithm.discount 0.9 --rl.evaluation_steps:EnvironmentEpisodes 5 --training_epochs 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#033[93mWarning: failed to import the following packages - tensorflow#033[0m\u001b[0m\n",
      "\u001b[34mApplying RL hyperparameter rl.improve_steps:TrainingSteps=100000\u001b[0m\n",
      "\u001b[34mApplying RL hyperparameter rl.agent_params.algorithm.discount=0.9\u001b[0m\n",
      "\u001b[34mApplying RL hyperparameter rl.evaluation_steps:EnvironmentEpisodes=5\u001b[0m\n",
      "\u001b[34mApplying RL hyperparameter rl.agent_params.algorithm.optimization_epochs=10\u001b[0m\n",
      "\u001b[34mLoading preset preset-portfolio-management-clippedppo from /opt/ml/code\u001b[0m\n",
      "\u001b[34m## Creating graph - name: BasicRLGraphManager\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\u001b[0m\n",
      "\u001b[34m## Creating agent - name: agent\u001b[0m\n",
      "\u001b[34mRequested devices [gpu(0)] not available. Default to CPU context.\u001b[0m\n",
      "\u001b[34mRequested devices [gpu(0)] not available. Default to CPU context.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m## Starting to improve simple_rl_graph task index 0\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=1, Total reward=-0.83, Steps=506, Training iteration=0\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=2, Total reward=-0.7, Steps=1012, Training iteration=0\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=3, Total reward=-0.8, Steps=1518, Training iteration=0\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=4, Total reward=-0.53, Steps=2024, Training iteration=0\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-0.47, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-0.43, Steps=2048, Training iteration=0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-0.46, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-0.42, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-0.33, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.42\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=5, Total reward=-0.92, Steps=2554, Training iteration=0\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/rl_coach/architectures/mxnet_components/heads/head.py:95: UserWarning: Parameter clippedppolosscontinuous0_kl_coefficient is not used by any computation. Is this intended?\n",
      "  outputs = super(HeadLoss, self).forward(*args)\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010547440499067307, KL divergence=[0.], Entropy=[-0.14192474], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0327768549323082, KL divergence=[0.], Entropy=[-0.1419121], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.057123828679323196, KL divergence=[0.], Entropy=[-0.14193949], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.07325004786252975, KL divergence=[0.], Entropy=[-0.14198118], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0827503502368927, KL divergence=[0.], Entropy=[-0.14194164], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.08976342529058456, KL divergence=[0.], Entropy=[-0.14192384], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.09522148966789246, KL divergence=[0.], Entropy=[-0.14191575], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.09993700683116913, KL divergence=[0.], Entropy=[-0.14191094], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.10410933941602707, KL divergence=[0.], Entropy=[-0.14191851], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.1075565293431282, KL divergence=[0.], Entropy=[-0.14190833], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/0_Step-2554.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=6, Total reward=-0.9, Steps=3060, Training iteration=1\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=7, Total reward=-1.03, Steps=3566, Training iteration=1\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=8, Total reward=-1.09, Steps=4072, Training iteration=1\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.46, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.46, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.37, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.36, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.52, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.43\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=9, Total reward=-1.01, Steps=4602, Training iteration=1\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012323670089244843, KL divergence=[0.], Entropy=[-0.14189252], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03049447387456894, KL divergence=[0.], Entropy=[-0.14185625], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.030128709971904755, KL divergence=[0.], Entropy=[-0.14183065], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.04538822919130325, KL divergence=[0.], Entropy=[-0.14181234], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.04183477535843849, KL divergence=[0.], Entropy=[-0.14181618], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.048290401697158813, KL divergence=[0.], Entropy=[-0.14180514], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.04508654400706291, KL divergence=[0.], Entropy=[-0.14180148], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.045979395508766174, KL divergence=[0.], Entropy=[-0.14179714], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.05486535280942917, KL divergence=[0.], Entropy=[-0.14178215], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.04915580525994301, KL divergence=[0.], Entropy=[-0.14177822], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/1_Step-4602.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=10, Total reward=-1.25, Steps=5108, Training iteration=2\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=11, Total reward=-1.07, Steps=5614, Training iteration=2\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=12, Total reward=-1.49, Steps=6120, Training iteration=2\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.28, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.2, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.27, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.31, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.26, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.26\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=13, Total reward=-1.31, Steps=6650, Training iteration=2\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0015988738741725683, KL divergence=[0.], Entropy=[-0.14175613], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01012580655515194, KL divergence=[0.], Entropy=[-0.14174335], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023384245112538338, KL divergence=[0.], Entropy=[-0.14173652], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020349886268377304, KL divergence=[0.], Entropy=[-0.14174406], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.026693500578403473, KL divergence=[0.], Entropy=[-0.14172003], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.024439115077257156, KL divergence=[0.], Entropy=[-0.14169694], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03321385756134987, KL divergence=[0.], Entropy=[-0.1416832], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03394271433353424, KL divergence=[0.], Entropy=[-0.14166854], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03486807644367218, KL divergence=[0.], Entropy=[-0.14165528], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.036283429712057114, KL divergence=[0.], Entropy=[-0.14164865], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/2_Step-6650.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=14, Total reward=-1.79, Steps=7156, Training iteration=3\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=15, Total reward=-1.2, Steps=7662, Training iteration=3\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=16, Total reward=-1.4, Steps=8168, Training iteration=3\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.14, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.09, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.05, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.03, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.04, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.07\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=17, Total reward=-1.16, Steps=8698, Training iteration=3\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00010306124022463337, KL divergence=[0.], Entropy=[-0.14164205], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00034315703669562936, KL divergence=[0.], Entropy=[-0.14164944], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008970691822469234, KL divergence=[0.], Entropy=[-0.14165916], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015099750831723213, KL divergence=[0.], Entropy=[-0.14165086], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019730132073163986, KL divergence=[0.], Entropy=[-0.14164868], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018157269805669785, KL divergence=[0.], Entropy=[-0.1416562], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02245575562119484, KL divergence=[0.], Entropy=[-0.14165162], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018879996612668037, KL divergence=[0.], Entropy=[-0.14163157], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.026036161929368973, KL divergence=[0.], Entropy=[-0.14162947], training epoch=8, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=-0.02623504027724266, KL divergence=[0.], Entropy=[-0.14161673], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/3_Step-8698.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=18, Total reward=-1.37, Steps=9204, Training iteration=4\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=19, Total reward=-1.69, Steps=9710, Training iteration=4\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=20, Total reward=-1.54, Steps=10216, Training iteration=4\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=0.14, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=0.02, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=-0.04, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=-0.03, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=0.02, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.02\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=21, Total reward=-1.76, Steps=10746, Training iteration=4\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0005450700409710407, KL divergence=[0.], Entropy=[-0.14159961], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00016813629190437496, KL divergence=[0.], Entropy=[-0.14161678], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009257415309548378, KL divergence=[0.], Entropy=[-0.14162579], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012265588156878948, KL divergence=[0.], Entropy=[-0.14163162], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014573176391422749, KL divergence=[0.], Entropy=[-0.1416396], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018581081181764603, KL divergence=[0.], Entropy=[-0.14163703], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02063077501952648, KL divergence=[0.], Entropy=[-0.14164165], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020466063171625137, KL divergence=[0.], Entropy=[-0.14163512], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01653493382036686, KL divergence=[0.], Entropy=[-0.14163281], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.027068493887782097, KL divergence=[0.], Entropy=[-0.1416189], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/4_Step-10746.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=22, Total reward=-1.23, Steps=11252, Training iteration=5\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=23, Total reward=-1.42, Steps=11758, Training iteration=5\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=24, Total reward=-1.41, Steps=12264, Training iteration=5\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.05, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.1, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.01, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.04, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=0.09, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.02\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=25, Total reward=-1.63, Steps=12794, Training iteration=5\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0005010378663428128, KL divergence=[0.], Entropy=[-0.14162229], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008438168093562126, KL divergence=[0.], Entropy=[-0.14166276], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007539109792560339, KL divergence=[0.], Entropy=[-0.14168291], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017604853957891464, KL divergence=[0.], Entropy=[-0.14170091], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00830699224025011, KL divergence=[0.], Entropy=[-0.14172195], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010049468837678432, KL divergence=[0.], Entropy=[-0.14173777], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017541591078042984, KL divergence=[0.], Entropy=[-0.14175124], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.024245625361800194, KL divergence=[0.], Entropy=[-0.14176275], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02374730072915554, KL divergence=[0.], Entropy=[-0.1417743], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022014040499925613, KL divergence=[0.], Entropy=[-0.14179082], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/5_Step-12794.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=26, Total reward=-1.56, Steps=13300, Training iteration=6\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=27, Total reward=-1.32, Steps=13806, Training iteration=6\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=28, Total reward=-1.67, Steps=14312, Training iteration=6\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=0.08, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=0.17, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=0.1, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=0.08, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=0.11, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.11\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=29, Total reward=-1.51, Steps=14842, Training iteration=6\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007378028240054846, KL divergence=[0.], Entropy=[-0.14180423], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008110947906970978, KL divergence=[0.], Entropy=[-0.14184529], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010719304904341698, KL divergence=[0.], Entropy=[-0.14186907], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021077563986182213, KL divergence=[0.], Entropy=[-0.14189483], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016251215711236, KL divergence=[0.], Entropy=[-0.1419039], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020142925903201103, KL divergence=[0.], Entropy=[-0.1419187], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01965607888996601, KL divergence=[0.], Entropy=[-0.14195003], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.025399871170520782, KL divergence=[0.], Entropy=[-0.1419664], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02019535005092621, KL divergence=[0.], Entropy=[-0.14197981], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.025446197018027306, KL divergence=[0.], Entropy=[-0.14199172], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/6_Step-14842.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=30, Total reward=-1.45, Steps=15348, Training iteration=7\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=31, Total reward=-1.0, Steps=15854, Training iteration=7\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=32, Total reward=-1.19, Steps=16360, Training iteration=7\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=0.02, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=0.22, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=0.12, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=0.12, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=0.11, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.12\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=33, Total reward=-2.03, Steps=16890, Training iteration=7\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0008991437498480082, KL divergence=[0.], Entropy=[-0.14200184], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008755446411669254, KL divergence=[0.], Entropy=[-0.14201245], training epoch=1, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=-0.007005932275205851, KL divergence=[0.], Entropy=[-0.14201722], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003621792420744896, KL divergence=[0.], Entropy=[-0.14203246], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019075430929660797, KL divergence=[0.], Entropy=[-0.14203113], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021277550607919693, KL divergence=[0.], Entropy=[-0.14203559], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014605460688471794, KL divergence=[0.], Entropy=[-0.14204189], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.026040339842438698, KL divergence=[0.], Entropy=[-0.14204447], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020529912784695625, KL divergence=[0.], Entropy=[-0.14205414], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021286213770508766, KL divergence=[0.], Entropy=[-0.14206873], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/7_Step-16890.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=34, Total reward=-1.08, Steps=17396, Training iteration=8\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=35, Total reward=-1.31, Steps=17902, Training iteration=8\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=36, Total reward=-1.62, Steps=18408, Training iteration=8\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=0.44, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=0.36, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=0.23, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=0.19, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=0.35, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.31\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=37, Total reward=-1.34, Steps=18938, Training iteration=8\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004494802560657263, KL divergence=[0.], Entropy=[-0.14207338], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00898743700236082, KL divergence=[0.], Entropy=[-0.14208469], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012392192147672176, KL divergence=[0.], Entropy=[-0.14208359], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012331330217421055, KL divergence=[0.], Entropy=[-0.14209232], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014659666456282139, KL divergence=[0.], Entropy=[-0.14209193], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016203952953219414, KL divergence=[0.], Entropy=[-0.14210032], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01831219717860222, KL divergence=[0.], Entropy=[-0.14209682], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020808745175600052, KL divergence=[0.], Entropy=[-0.14208744], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022486291825771332, KL divergence=[0.], Entropy=[-0.14206988], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022406956180930138, KL divergence=[0.], Entropy=[-0.14208038], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/8_Step-18938.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=38, Total reward=-1.1, Steps=19444, Training iteration=9\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=39, Total reward=-1.38, Steps=19950, Training iteration=9\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=40, Total reward=-1.46, Steps=20456, Training iteration=9\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=0.38, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=0.51, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=0.44, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=0.34, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=0.4, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.41\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=41, Total reward=-1.04, Steps=20986, Training iteration=9\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00046002527233213186, KL divergence=[0.], Entropy=[-0.1420709], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004314715508371592, KL divergence=[0.], Entropy=[-0.14206693], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00349008128978312, KL divergence=[0.], Entropy=[-0.14205644], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016838504001498222, KL divergence=[0.], Entropy=[-0.14203951], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013220460154116154, KL divergence=[0.], Entropy=[-0.14202297], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022809728980064392, KL divergence=[0.], Entropy=[-0.1420057], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020055832341313362, KL divergence=[0.], Entropy=[-0.14198884], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02318628877401352, KL divergence=[0.], Entropy=[-0.14197466], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022842349484562874, KL divergence=[0.], Entropy=[-0.14195046], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02671784907579422, KL divergence=[0.], Entropy=[-0.14194372], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/9_Step-20986.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=42, Total reward=-1.43, Steps=21492, Training iteration=10\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=43, Total reward=-1.13, Steps=21998, Training iteration=10\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=44, Total reward=-1.26, Steps=22504, Training iteration=10\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=0.64, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=0.58, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=0.45, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=0.41, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=0.52, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.52\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=45, Total reward=-1.87, Steps=23034, Training iteration=10\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002483865711838007, KL divergence=[0.], Entropy=[-0.14195131], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008283283561468124, KL divergence=[0.], Entropy=[-0.14197178], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013762335292994976, KL divergence=[0.], Entropy=[-0.14199984], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013917859643697739, KL divergence=[0.], Entropy=[-0.14203192], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016976747661828995, KL divergence=[0.], Entropy=[-0.14206512], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01852968893945217, KL divergence=[0.], Entropy=[-0.142085], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01987559162080288, KL divergence=[0.], Entropy=[-0.14210503], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016839811578392982, KL divergence=[0.], Entropy=[-0.14212227], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021540865302085876, KL divergence=[0.], Entropy=[-0.14213192], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.024263037368655205, KL divergence=[0.], Entropy=[-0.14214319], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/10_Step-23034.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=46, Total reward=-1.37, Steps=23540, Training iteration=11\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=47, Total reward=-1.57, Steps=24046, Training iteration=11\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=48, Total reward=-1.17, Steps=24552, Training iteration=11\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=0.5, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=0.47, Steps=24576, Training iteration=11\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=0.51, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=0.59, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=0.52, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.52\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=49, Total reward=-1.2, Steps=25082, Training iteration=11\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001983839785680175, KL divergence=[0.], Entropy=[-0.14214613], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004103517159819603, KL divergence=[0.], Entropy=[-0.14214794], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01275541540235281, KL divergence=[0.], Entropy=[-0.14214599], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014273268170654774, KL divergence=[0.], Entropy=[-0.14216028], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015997495502233505, KL divergence=[0.], Entropy=[-0.1421664], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012149806134402752, KL divergence=[0.], Entropy=[-0.14216802], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022613009437918663, KL divergence=[0.], Entropy=[-0.14216675], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018889354541897774, KL divergence=[0.], Entropy=[-0.14217325], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017753086984157562, KL divergence=[0.], Entropy=[-0.14217493], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.024498624727129936, KL divergence=[0.], Entropy=[-0.14217216], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/11_Step-25082.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=50, Total reward=-1.37, Steps=25588, Training iteration=12\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=51, Total reward=-1.38, Steps=26094, Training iteration=12\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=52, Total reward=-0.7, Steps=26600, Training iteration=12\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=0.63, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=0.8, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=0.63, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=0.59, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=0.56, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.64\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=53, Total reward=-0.96, Steps=27130, Training iteration=12\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0018750878516584635, KL divergence=[0.], Entropy=[-0.14216994], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004735537804663181, KL divergence=[0.], Entropy=[-0.14217997], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012905478477478027, KL divergence=[0.], Entropy=[-0.14218885], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00998406857252121, KL divergence=[0.], Entropy=[-0.142208], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014094797894358635, KL divergence=[0.], Entropy=[-0.14221312], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019047478213906288, KL divergence=[0.], Entropy=[-0.14221983], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018413932994008064, KL divergence=[0.], Entropy=[-0.1422163], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023281574249267578, KL divergence=[0.], Entropy=[-0.14221619], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023553675040602684, KL divergence=[0.], Entropy=[-0.14221701], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02630254067480564, KL divergence=[0.], Entropy=[-0.14221197], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/12_Step-27130.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=54, Total reward=-1.22, Steps=27636, Training iteration=13\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=55, Total reward=-1.29, Steps=28142, Training iteration=13\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=56, Total reward=-1.44, Steps=28648, Training iteration=13\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=0.8, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=0.65, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=0.68, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=0.62, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=0.68, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.69\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=57, Total reward=-1.21, Steps=29178, Training iteration=13\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002576313214376569, KL divergence=[0.], Entropy=[-0.1422254], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012616773135960102, KL divergence=[0.], Entropy=[-0.14224474], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010072031058371067, KL divergence=[0.], Entropy=[-0.14226146], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011051979847252369, KL divergence=[0.], Entropy=[-0.14227168], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01727701723575592, KL divergence=[0.], Entropy=[-0.14227965], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01753729023039341, KL divergence=[0.], Entropy=[-0.14229064], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018909795209765434, KL divergence=[0.], Entropy=[-0.1422971], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018946435302495956, KL divergence=[0.], Entropy=[-0.1422895], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02103468030691147, KL divergence=[0.], Entropy=[-0.14229967], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022579487413167953, KL divergence=[0.], Entropy=[-0.14230306], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/13_Step-29178.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=58, Total reward=-1.46, Steps=29684, Training iteration=14\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=59, Total reward=-1.35, Steps=30190, Training iteration=14\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=60, Total reward=-1.1, Steps=30696, Training iteration=14\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=0.75, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=0.66, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=0.73, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=0.67, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=0.82, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.73\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=61, Total reward=-1.19, Steps=31226, Training iteration=14\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004311176482588053, KL divergence=[0.], Entropy=[-0.14230321], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0037603152450174093, KL divergence=[0.], Entropy=[-0.1423108], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008461219258606434, KL divergence=[0.], Entropy=[-0.14232041], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008581995964050293, KL divergence=[0.], Entropy=[-0.14233574], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018530212342739105, KL divergence=[0.], Entropy=[-0.14234425], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013197219930589199, KL divergence=[0.], Entropy=[-0.14236684], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013059916906058788, KL divergence=[0.], Entropy=[-0.14237015], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01655958965420723, KL divergence=[0.], Entropy=[-0.14238492], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017785411328077316, KL divergence=[0.], Entropy=[-0.14238499], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015235097147524357, KL divergence=[0.], Entropy=[-0.14239334], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/14_Step-31226.ckpt.main_level.agent.main.online']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=62, Total reward=-1.28, Steps=31732, Training iteration=15\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=63, Total reward=-0.94, Steps=32238, Training iteration=15\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=64, Total reward=-0.84, Steps=32744, Training iteration=15\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=0.68, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=0.73, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=0.73, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=0.81, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=0.59, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.71\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=65, Total reward=-1.5, Steps=33274, Training iteration=15\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005907056853175163, KL divergence=[0.], Entropy=[-0.14238505], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00755732087418437, KL divergence=[0.], Entropy=[-0.14236057], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011453976854681969, KL divergence=[0.], Entropy=[-0.1423414], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010724766179919243, KL divergence=[0.], Entropy=[-0.14233385], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014444172382354736, KL divergence=[0.], Entropy=[-0.14231749], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018003668636083603, KL divergence=[0.], Entropy=[-0.14230558], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015054335817694664, KL divergence=[0.], Entropy=[-0.14229237], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01662910170853138, KL divergence=[0.], Entropy=[-0.14227493], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012909626588225365, KL divergence=[0.], Entropy=[-0.14226633], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018806476145982742, KL divergence=[0.], Entropy=[-0.14225626], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/15_Step-33274.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=66, Total reward=-1.24, Steps=33780, Training iteration=16\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=67, Total reward=-1.56, Steps=34286, Training iteration=16\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=68, Total reward=-1.24, Steps=34792, Training iteration=16\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=0.94, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=0.83, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=0.9, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=0.94, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=0.86, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.9\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=69, Total reward=-1.19, Steps=35322, Training iteration=16\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0046939249150455, KL divergence=[0.], Entropy=[-0.14225192], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00724767753854394, KL divergence=[0.], Entropy=[-0.14224274], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006918840575963259, KL divergence=[0.], Entropy=[-0.14224221], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01281802263110876, KL divergence=[0.], Entropy=[-0.14224209], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01504603587090969, KL divergence=[0.], Entropy=[-0.1422349], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016544513404369354, KL divergence=[0.], Entropy=[-0.14223234], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017220821231603622, KL divergence=[0.], Entropy=[-0.14223479], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.027531420812010765, KL divergence=[0.], Entropy=[-0.14223602], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021782634779810905, KL divergence=[0.], Entropy=[-0.14223677], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019951699301600456, KL divergence=[0.], Entropy=[-0.14222974], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/16_Step-35322.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=70, Total reward=-1.17, Steps=35828, Training iteration=17\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=71, Total reward=-1.5, Steps=36334, Training iteration=17\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=72, Total reward=-1.13, Steps=36840, Training iteration=17\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=1.03, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=0.95, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=0.89, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=0.85, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=1.09, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.96\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=73, Total reward=-1.27, Steps=37370, Training iteration=17\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004681374877691269, KL divergence=[0.], Entropy=[-0.14223781], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006477893330156803, KL divergence=[0.], Entropy=[-0.14225778], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009968622587621212, KL divergence=[0.], Entropy=[-0.14227563], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0103905089199543, KL divergence=[0.], Entropy=[-0.14228155], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01578366570174694, KL divergence=[0.], Entropy=[-0.14229691], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013258890248835087, KL divergence=[0.], Entropy=[-0.14230213], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019358104094862938, KL divergence=[0.], Entropy=[-0.14231707], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014192367903888226, KL divergence=[0.], Entropy=[-0.14232527], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023181501775979996, KL divergence=[0.], Entropy=[-0.14232402], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01923675835132599, KL divergence=[0.], Entropy=[-0.14233334], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/17_Step-37370.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=74, Total reward=-1.03, Steps=37876, Training iteration=18\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=75, Total reward=-0.89, Steps=38382, Training iteration=18\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=76, Total reward=-1.16, Steps=38888, Training iteration=18\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=1.05, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=0.96, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=0.98, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=0.96, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=1.11, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.01\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=77, Total reward=-0.65, Steps=39418, Training iteration=18\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004038513172417879, KL divergence=[0.], Entropy=[-0.14233854], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006813691928982735, KL divergence=[0.], Entropy=[-0.1423295], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008722292259335518, KL divergence=[0.], Entropy=[-0.14231937], training epoch=2, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=-0.013153470121324062, KL divergence=[0.], Entropy=[-0.14230737], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012171844951808453, KL divergence=[0.], Entropy=[-0.14229532], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009883696213364601, KL divergence=[0.], Entropy=[-0.14228271], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018662920221686363, KL divergence=[0.], Entropy=[-0.14227077], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017675038427114487, KL divergence=[0.], Entropy=[-0.1422619], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021134329959750175, KL divergence=[0.], Entropy=[-0.14225079], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01719995029270649, KL divergence=[0.], Entropy=[-0.14223252], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/18_Step-39418.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=78, Total reward=-1.09, Steps=39924, Training iteration=19\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=79, Total reward=-1.65, Steps=40430, Training iteration=19\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=80, Total reward=-1.39, Steps=40936, Training iteration=19\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=0.98, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=1.07, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=1.14, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=1.03, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=1.08, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.06\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=81, Total reward=-0.91, Steps=41466, Training iteration=19\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0010037149768322706, KL divergence=[0.], Entropy=[-0.1422282], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004253302235156298, KL divergence=[0.], Entropy=[-0.14223577], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007355591747909784, KL divergence=[0.], Entropy=[-0.14226574], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007952094078063965, KL divergence=[0.], Entropy=[-0.14230712], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003057020716369152, KL divergence=[0.], Entropy=[-0.14233214], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01072804443538189, KL divergence=[0.], Entropy=[-0.14235857], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013174345716834068, KL divergence=[0.], Entropy=[-0.14236608], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011255765333771706, KL divergence=[0.], Entropy=[-0.14237161], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018101820722222328, KL divergence=[0.], Entropy=[-0.14238618], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019580569118261337, KL divergence=[0.], Entropy=[-0.14238717], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/19_Step-41466.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=82, Total reward=-1.08, Steps=41972, Training iteration=20\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=83, Total reward=-1.41, Steps=42478, Training iteration=20\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=84, Total reward=-1.24, Steps=42984, Training iteration=20\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=1.11, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=1.11, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=1.23, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=1.09, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=1.03, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.11\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=85, Total reward=-0.87, Steps=43514, Training iteration=20\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002274621743708849, KL divergence=[0.], Entropy=[-0.14239004], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005007075611501932, KL divergence=[0.], Entropy=[-0.14240734], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00964718870818615, KL divergence=[0.], Entropy=[-0.14240201], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006972767878323793, KL divergence=[0.], Entropy=[-0.14239895], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011009937152266502, KL divergence=[0.], Entropy=[-0.1424015], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016110096126794815, KL divergence=[0.], Entropy=[-0.14241505], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01608293503522873, KL divergence=[0.], Entropy=[-0.14240824], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010851654224097729, KL divergence=[0.], Entropy=[-0.14240871], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013973413966596127, KL divergence=[0.], Entropy=[-0.14241646], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018408656120300293, KL divergence=[0.], Entropy=[-0.14241372], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/20_Step-43514.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=86, Total reward=-1.14, Steps=44020, Training iteration=21\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=87, Total reward=-0.97, Steps=44526, Training iteration=21\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=88, Total reward=-0.95, Steps=45032, Training iteration=21\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=1.11, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=1.14, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=1.21, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=1.15, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=1.18, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.16\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=89, Total reward=-1.12, Steps=45562, Training iteration=21\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0024325030390173197, KL divergence=[0.], Entropy=[-0.14239869], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0012599090114235878, KL divergence=[0.], Entropy=[-0.1423758], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010014355182647705, KL divergence=[0.], Entropy=[-0.14235784], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01537271123379469, KL divergence=[0.], Entropy=[-0.1423279], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011870517395436764, KL divergence=[0.], Entropy=[-0.14231579], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01489220466464758, KL divergence=[0.], Entropy=[-0.14230593], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014750201255083084, KL divergence=[0.], Entropy=[-0.14229122], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018907738849520683, KL divergence=[0.], Entropy=[-0.14227997], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01982727460563183, KL divergence=[0.], Entropy=[-0.14227], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018964039161801338, KL divergence=[0.], Entropy=[-0.14226688], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/21_Step-45562.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=90, Total reward=-0.7, Steps=46068, Training iteration=22\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=91, Total reward=-0.99, Steps=46574, Training iteration=22\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=92, Total reward=-1.06, Steps=47080, Training iteration=22\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=1.2, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=1.31, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=1.22, Steps=47104, Training iteration=22\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=1.36, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=1.35, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.29\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=93, Total reward=-1.1, Steps=47610, Training iteration=22\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00314492080360651, KL divergence=[0.], Entropy=[-0.1422726], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004309006500989199, KL divergence=[0.], Entropy=[-0.14229551], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002759566530585289, KL divergence=[0.], Entropy=[-0.14229871], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008709232322871685, KL divergence=[0.], Entropy=[-0.14230798], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006329235155135393, KL divergence=[0.], Entropy=[-0.14232832], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010260598734021187, KL divergence=[0.], Entropy=[-0.14232832], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011699804104864597, KL divergence=[0.], Entropy=[-0.14233203], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010940107516944408, KL divergence=[0.], Entropy=[-0.14232773], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015044309198856354, KL divergence=[0.], Entropy=[-0.14231996], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020667539909482002, KL divergence=[0.], Entropy=[-0.14232361], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/22_Step-47610.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=94, Total reward=-0.91, Steps=48116, Training iteration=23\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=95, Total reward=-0.94, Steps=48622, Training iteration=23\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=96, Total reward=-0.98, Steps=49128, Training iteration=23\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=1.29, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=1.35, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=1.27, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=1.3, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=1.26, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.29\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=97, Total reward=-0.55, Steps=49658, Training iteration=23\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004131322726607323, KL divergence=[0.], Entropy=[-0.14232574], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0028998188208788633, KL divergence=[0.], Entropy=[-0.14236115], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009710260666906834, KL divergence=[0.], Entropy=[-0.14238767], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0052810111083090305, KL divergence=[0.], Entropy=[-0.14240667], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012388304807245731, KL divergence=[0.], Entropy=[-0.14243355], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009797897189855576, KL divergence=[0.], Entropy=[-0.14243962], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0122143579646945, KL divergence=[0.], Entropy=[-0.14243966], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010969751514494419, KL divergence=[0.], Entropy=[-0.14244726], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006611419375985861, KL divergence=[0.], Entropy=[-0.14243673], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01114277821034193, KL divergence=[0.], Entropy=[-0.14244919], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/23_Step-49658.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=98, Total reward=-0.98, Steps=50164, Training iteration=24\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=99, Total reward=-0.72, Steps=50670, Training iteration=24\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=100, Total reward=-0.98, Steps=51176, Training iteration=24\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=1.29, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=1.32, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=1.32, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=1.31, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=1.31, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.31\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=101, Total reward=-0.79, Steps=51706, Training iteration=24\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0033436384983360767, KL divergence=[0.], Entropy=[-0.14246173], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00329743605107069, KL divergence=[0.], Entropy=[-0.14246319], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005846584215760231, KL divergence=[0.], Entropy=[-0.14247748], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010688530281186104, KL divergence=[0.], Entropy=[-0.14247735], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008616647683084011, KL divergence=[0.], Entropy=[-0.1424861], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008944625034928322, KL divergence=[0.], Entropy=[-0.14248668], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011724337004125118, KL divergence=[0.], Entropy=[-0.14248845], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01564469002187252, KL divergence=[0.], Entropy=[-0.14247662], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01363329403102398, KL divergence=[0.], Entropy=[-0.14247715], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008764415048062801, KL divergence=[0.], Entropy=[-0.14245994], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/24_Step-51706.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=102, Total reward=-1.38, Steps=52212, Training iteration=25\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=103, Total reward=-1.05, Steps=52718, Training iteration=25\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=104, Total reward=-0.92, Steps=53224, Training iteration=25\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=1.47, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=1.39, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=1.39, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=1.28, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=1.43, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.39\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=105, Total reward=-1.39, Steps=53754, Training iteration=25\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=-0.0028276911471039057, KL divergence=[0.], Entropy=[-0.14246039], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005137464497238398, KL divergence=[0.], Entropy=[-0.14247449], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01059898640960455, KL divergence=[0.], Entropy=[-0.14246121], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002816257532685995, KL divergence=[0.], Entropy=[-0.14245757], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006480109412223101, KL divergence=[0.], Entropy=[-0.1424505], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014662773348391056, KL divergence=[0.], Entropy=[-0.1424432], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006662333384156227, KL divergence=[0.], Entropy=[-0.14243408], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017244426533579826, KL divergence=[0.], Entropy=[-0.14243056], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010119074955582619, KL divergence=[0.], Entropy=[-0.1424121], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008024505339562893, KL divergence=[0.], Entropy=[-0.14241816], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/25_Step-53754.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=106, Total reward=-0.9, Steps=54260, Training iteration=26\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=107, Total reward=-1.16, Steps=54766, Training iteration=26\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=108, Total reward=-1.37, Steps=55272, Training iteration=26\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=1.48, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=1.57, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=1.51, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=1.5, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=1.4, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.49\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=109, Total reward=-0.88, Steps=55802, Training iteration=26\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0004341736785136163, KL divergence=[0.], Entropy=[-0.14241733], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004268554970622063, KL divergence=[0.], Entropy=[-0.14242336], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0022258537355810404, KL divergence=[0.], Entropy=[-0.1424391], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002838323125615716, KL divergence=[0.], Entropy=[-0.14245127], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0030735707841813564, KL divergence=[0.], Entropy=[-0.14244121], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007468030788004398, KL divergence=[0.], Entropy=[-0.14244187], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007767734117805958, KL divergence=[0.], Entropy=[-0.14244626], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011881214566528797, KL divergence=[0.], Entropy=[-0.14245377], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010001899674534798, KL divergence=[0.], Entropy=[-0.1424507], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009260387159883976, KL divergence=[0.], Entropy=[-0.14244969], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/26_Step-55802.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=110, Total reward=-1.11, Steps=56308, Training iteration=27\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=111, Total reward=-1.09, Steps=56814, Training iteration=27\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=112, Total reward=-0.91, Steps=57320, Training iteration=27\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=1.3, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=1.43, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=1.53, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=1.6, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=1.49, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.47\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=113, Total reward=-1.12, Steps=57850, Training iteration=27\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0008243665797635913, KL divergence=[0.], Entropy=[-0.14247063], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0003255596093367785, KL divergence=[0.], Entropy=[-0.14249445], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003173437435179949, KL divergence=[0.], Entropy=[-0.1425254], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005349340848624706, KL divergence=[0.], Entropy=[-0.14252532], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002184975193813443, KL divergence=[0.], Entropy=[-0.14252621], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005174474790692329, KL divergence=[0.], Entropy=[-0.14252068], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005780588835477829, KL divergence=[0.], Entropy=[-0.14251779], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005748755764216185, KL divergence=[0.], Entropy=[-0.1425331], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01362626627087593, KL divergence=[0.], Entropy=[-0.14253905], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015944065526127815, KL divergence=[0.], Entropy=[-0.14253508], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/27_Step-57850.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=114, Total reward=-0.89, Steps=58356, Training iteration=28\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=115, Total reward=-0.94, Steps=58862, Training iteration=28\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=116, Total reward=-0.49, Steps=59368, Training iteration=28\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=1.35, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=1.51, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=1.49, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=1.48, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=1.46, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.46\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=117, Total reward=-0.87, Steps=59898, Training iteration=28\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00112354033626616, KL divergence=[0.], Entropy=[-0.14254984], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00637333607301116, KL divergence=[0.], Entropy=[-0.14257534], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007755276747047901, KL divergence=[0.], Entropy=[-0.14259554], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0037681753747165203, KL divergence=[0.], Entropy=[-0.14258802], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0018908546771854162, KL divergence=[0.], Entropy=[-0.14257105], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006611814256757498, KL divergence=[0.], Entropy=[-0.1425677], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006325175985693932, KL divergence=[0.], Entropy=[-0.14255933], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004456298891454935, KL divergence=[0.], Entropy=[-0.14256036], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005660658702254295, KL divergence=[0.], Entropy=[-0.14255513], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005316609051078558, KL divergence=[0.], Entropy=[-0.14255445], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/28_Step-59898.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=118, Total reward=-0.99, Steps=60404, Training iteration=29\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=119, Total reward=-1.27, Steps=60910, Training iteration=29\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=120, Total reward=-0.93, Steps=61416, Training iteration=29\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=1.5, Steps=61440, Training iteration=29\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=1.55, Steps=61440, Training iteration=29\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=1.41, Steps=61440, Training iteration=29\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=1.53, Steps=61440, Training iteration=29\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=1.54, Steps=61440, Training iteration=29\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.51\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=121, Total reward=-0.79, Steps=61946, Training iteration=29\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003128254320472479, KL divergence=[0.], Entropy=[-0.14255458], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0007790644303895533, KL divergence=[0.], Entropy=[-0.14254074], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0019689586479216814, KL divergence=[0.], Entropy=[-0.14255063], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0014755682786926627, KL divergence=[0.], Entropy=[-0.1425495], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003600180149078369, KL divergence=[0.], Entropy=[-0.14253217], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004949183203279972, KL divergence=[0.], Entropy=[-0.14252391], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006346603389829397, KL divergence=[0.], Entropy=[-0.14255002], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006461737211793661, KL divergence=[0.], Entropy=[-0.14254607], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0053139859810471535, KL divergence=[0.], Entropy=[-0.14253566], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0038141408003866673, KL divergence=[0.], Entropy=[-0.14252143], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/29_Step-61946.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=122, Total reward=-0.71, Steps=62452, Training iteration=30\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=123, Total reward=-1.03, Steps=62958, Training iteration=30\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=124, Total reward=-0.74, Steps=63464, Training iteration=30\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=124, Total reward=1.56, Steps=63488, Training iteration=30\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=124, Total reward=1.54, Steps=63488, Training iteration=30\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=124, Total reward=1.54, Steps=63488, Training iteration=30\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=124, Total reward=1.51, Steps=63488, Training iteration=30\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=124, Total reward=1.55, Steps=63488, Training iteration=30\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.54\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=125, Total reward=-1.03, Steps=63994, Training iteration=30\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00037794921081513166, KL divergence=[0.], Entropy=[-0.14252324], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002108787652105093, KL divergence=[0.], Entropy=[-0.14250356], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0019909990951418877, KL divergence=[0.], Entropy=[-0.14249891], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0033430405892431736, KL divergence=[0.], Entropy=[-0.14249457], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0021369056776165962, KL divergence=[0.], Entropy=[-0.1425026], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0009264222462661564, KL divergence=[0.], Entropy=[-0.14251211], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0038843967486172915, KL divergence=[0.], Entropy=[-0.14250559], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011352122761309147, KL divergence=[0.], Entropy=[-0.14250505], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007095544598996639, KL divergence=[0.], Entropy=[-0.14250508], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004313458688557148, KL divergence=[0.], Entropy=[-0.1425112], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/30_Step-63994.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=126, Total reward=-0.83, Steps=64500, Training iteration=31\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=127, Total reward=-0.98, Steps=65006, Training iteration=31\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=128, Total reward=-0.97, Steps=65512, Training iteration=31\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=128, Total reward=1.5, Steps=65536, Training iteration=31\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=128, Total reward=1.65, Steps=65536, Training iteration=31\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=128, Total reward=1.5, Steps=65536, Training iteration=31\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=128, Total reward=1.51, Steps=65536, Training iteration=31\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=128, Total reward=1.49, Steps=65536, Training iteration=31\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.53\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=129, Total reward=-1.12, Steps=66042, Training iteration=31\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002163839992135763, KL divergence=[0.], Entropy=[-0.14250897], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003069530474022031, KL divergence=[0.], Entropy=[-0.14250754], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0045284428633749485, KL divergence=[0.], Entropy=[-0.14251672], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00014815552276559174, KL divergence=[0.], Entropy=[-0.14252454], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001503295497968793, KL divergence=[0.], Entropy=[-0.1425158], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0008104147855192423, KL divergence=[0.], Entropy=[-0.14250904], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.001564834383316338, KL divergence=[0.], Entropy=[-0.14250647], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00014767087122891098, KL divergence=[0.], Entropy=[-0.14250703], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00018192155403085053, KL divergence=[0.], Entropy=[-0.14252292], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0015929535729810596, KL divergence=[0.], Entropy=[-0.14252886], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/31_Step-66042.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=130, Total reward=-1.0, Steps=66548, Training iteration=32\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=131, Total reward=-0.51, Steps=67054, Training iteration=32\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=132, Total reward=-1.06, Steps=67560, Training iteration=32\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=132, Total reward=1.63, Steps=67584, Training iteration=32\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=132, Total reward=1.58, Steps=67584, Training iteration=32\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=132, Total reward=1.65, Steps=67584, Training iteration=32\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=132, Total reward=1.48, Steps=67584, Training iteration=32\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=132, Total reward=1.58, Steps=67584, Training iteration=32\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.58\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=133, Total reward=-1.13, Steps=68090, Training iteration=32\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=7.625789294252172e-05, KL divergence=[0.], Entropy=[-0.14254184], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0014749435940757394, KL divergence=[0.], Entropy=[-0.14255284], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.001250442466698587, KL divergence=[0.], Entropy=[-0.14255056], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0012420716229826212, KL divergence=[0.], Entropy=[-0.14254515], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0015218048356473446, KL divergence=[0.], Entropy=[-0.14255421], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004495662171393633, KL divergence=[0.], Entropy=[-0.14256403], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0016657831147313118, KL divergence=[0.], Entropy=[-0.14255731], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00017974068759940565, KL divergence=[0.], Entropy=[-0.14257456], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.001643843948841095, KL divergence=[0.], Entropy=[-0.14257085], training epoch=8, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=0.005412551574409008, KL divergence=[0.], Entropy=[-0.14256948], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/32_Step-68090.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=134, Total reward=-0.73, Steps=68596, Training iteration=33\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=135, Total reward=-0.85, Steps=69102, Training iteration=33\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=136, Total reward=-1.12, Steps=69608, Training iteration=33\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=136, Total reward=1.61, Steps=69632, Training iteration=33\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=136, Total reward=1.66, Steps=69632, Training iteration=33\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=136, Total reward=1.61, Steps=69632, Training iteration=33\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=136, Total reward=1.6, Steps=69632, Training iteration=33\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=136, Total reward=1.61, Steps=69632, Training iteration=33\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.62\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=137, Total reward=-1.01, Steps=70138, Training iteration=33\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.006076876074075699, KL divergence=[0.], Entropy=[-0.14256774], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004940743558108807, KL divergence=[0.], Entropy=[-0.14258905], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0030261476058512926, KL divergence=[0.], Entropy=[-0.14258882], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0012228837003931403, KL divergence=[0.], Entropy=[-0.14259517], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00048313531442545354, KL divergence=[0.], Entropy=[-0.1426018], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0005731972050853074, KL divergence=[0.], Entropy=[-0.14259583], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.007806858513504267, KL divergence=[0.], Entropy=[-0.14260493], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0014111034106463194, KL divergence=[0.], Entropy=[-0.14259611], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.006261319853365421, KL divergence=[0.], Entropy=[-0.14260325], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0023510653991252184, KL divergence=[0.], Entropy=[-0.14259784], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/33_Step-70138.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=138, Total reward=-1.1, Steps=70644, Training iteration=34\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=139, Total reward=-1.27, Steps=71150, Training iteration=34\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=140, Total reward=-1.01, Steps=71656, Training iteration=34\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=140, Total reward=1.6, Steps=71680, Training iteration=34\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=140, Total reward=1.65, Steps=71680, Training iteration=34\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=140, Total reward=1.58, Steps=71680, Training iteration=34\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=140, Total reward=1.56, Steps=71680, Training iteration=34\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=140, Total reward=1.6, Steps=71680, Training iteration=34\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.6\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=141, Total reward=-0.69, Steps=72186, Training iteration=34\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004917897749692202, KL divergence=[0.], Entropy=[-0.14261983], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0013403021730482578, KL divergence=[0.], Entropy=[-0.14263812], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0013015676522627473, KL divergence=[0.], Entropy=[-0.142636], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0010225109290331602, KL divergence=[0.], Entropy=[-0.14265627], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0014040474779903889, KL divergence=[0.], Entropy=[-0.14263761], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0003511622198857367, KL divergence=[0.], Entropy=[-0.14262965], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0004854473809245974, KL divergence=[0.], Entropy=[-0.1426401], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0046607572585344315, KL divergence=[0.], Entropy=[-0.14264712], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0009092601831071079, KL divergence=[0.], Entropy=[-0.1426385], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0033531063236296177, KL divergence=[0.], Entropy=[-0.14263923], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/34_Step-72186.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=142, Total reward=-0.52, Steps=72692, Training iteration=35\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=143, Total reward=-0.94, Steps=73198, Training iteration=35\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=144, Total reward=-0.76, Steps=73704, Training iteration=35\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=144, Total reward=1.53, Steps=73728, Training iteration=35\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=144, Total reward=1.56, Steps=73728, Training iteration=35\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=144, Total reward=1.68, Steps=73728, Training iteration=35\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=144, Total reward=1.62, Steps=73728, Training iteration=35\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=144, Total reward=1.56, Steps=73728, Training iteration=35\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.59\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=145, Total reward=-1.3, Steps=74234, Training iteration=35\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0007950827712193131, KL divergence=[0.], Entropy=[-0.1426428], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0045815506018698215, KL divergence=[0.], Entropy=[-0.14266364], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0024140062741935253, KL divergence=[0.], Entropy=[-0.1426454], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=3.790038317674771e-05, KL divergence=[0.], Entropy=[-0.14265403], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0005276568699628115, KL divergence=[0.], Entropy=[-0.14263973], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001588079729117453, KL divergence=[0.], Entropy=[-0.14265247], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0005455574719235301, KL divergence=[0.], Entropy=[-0.14265794], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00616734242066741, KL divergence=[0.], Entropy=[-0.14266138], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0007672617794014513, KL divergence=[0.], Entropy=[-0.14266258], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0001500912185292691, KL divergence=[0.], Entropy=[-0.1426547], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/35_Step-74234.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=146, Total reward=-1.04, Steps=74740, Training iteration=36\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=147, Total reward=-0.87, Steps=75246, Training iteration=36\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=148, Total reward=-1.44, Steps=75752, Training iteration=36\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=148, Total reward=1.55, Steps=75776, Training iteration=36\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=148, Total reward=1.64, Steps=75776, Training iteration=36\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=148, Total reward=1.59, Steps=75776, Training iteration=36\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=148, Total reward=1.71, Steps=75776, Training iteration=36\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=148, Total reward=1.61, Steps=75776, Training iteration=36\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.62\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=149, Total reward=-1.11, Steps=76282, Training iteration=36\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0066181086003780365, KL divergence=[0.], Entropy=[-0.14266548], training epoch=0, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=0.0037267920561134815, KL divergence=[0.], Entropy=[-0.1426698], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0006737239309586585, KL divergence=[0.], Entropy=[-0.14269388], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003821284044533968, KL divergence=[0.], Entropy=[-0.14268233], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0014837234048172832, KL divergence=[0.], Entropy=[-0.14269613], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0009913871763274074, KL divergence=[0.], Entropy=[-0.14270268], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0032081666868180037, KL divergence=[0.], Entropy=[-0.14269763], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0005074032815173268, KL divergence=[0.], Entropy=[-0.14269346], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0025401427410542965, KL divergence=[0.], Entropy=[-0.14269854], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0019303824519738555, KL divergence=[0.], Entropy=[-0.14268936], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/36_Step-76282.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=150, Total reward=-1.1, Steps=76788, Training iteration=37\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=151, Total reward=-0.58, Steps=77294, Training iteration=37\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=152, Total reward=-0.9, Steps=77800, Training iteration=37\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=152, Total reward=1.67, Steps=77824, Training iteration=37\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=152, Total reward=1.66, Steps=77824, Training iteration=37\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=152, Total reward=1.53, Steps=77824, Training iteration=37\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=152, Total reward=1.63, Steps=77824, Training iteration=37\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=152, Total reward=1.75, Steps=77824, Training iteration=37\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.65\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=153, Total reward=-0.93, Steps=78330, Training iteration=37\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0016754122916609049, KL divergence=[0.], Entropy=[-0.14270107], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001184153719805181, KL divergence=[0.], Entropy=[-0.14270143], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004875131882727146, KL divergence=[0.], Entropy=[-0.14269713], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00038512772880494595, KL divergence=[0.], Entropy=[-0.14271155], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0019179421942681074, KL divergence=[0.], Entropy=[-0.14272018], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001320311101153493, KL divergence=[0.], Entropy=[-0.14271756], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0013093609595671296, KL divergence=[0.], Entropy=[-0.14271817], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0023957928642630577, KL divergence=[0.], Entropy=[-0.14272097], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0033405206631869078, KL divergence=[0.], Entropy=[-0.14272748], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0009309941087849438, KL divergence=[0.], Entropy=[-0.14273007], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/37_Step-78330.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=154, Total reward=-1.15, Steps=78836, Training iteration=38\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=155, Total reward=-1.07, Steps=79342, Training iteration=38\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=156, Total reward=-0.76, Steps=79848, Training iteration=38\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=156, Total reward=1.69, Steps=79872, Training iteration=38\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=156, Total reward=1.65, Steps=79872, Training iteration=38\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=156, Total reward=1.56, Steps=79872, Training iteration=38\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=156, Total reward=1.64, Steps=79872, Training iteration=38\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=156, Total reward=1.69, Steps=79872, Training iteration=38\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.64\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=157, Total reward=-0.9, Steps=80378, Training iteration=38\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00638835784047842, KL divergence=[0.], Entropy=[-0.14273034], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0029358461033552885, KL divergence=[0.], Entropy=[-0.14273165], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0030247201211750507, KL divergence=[0.], Entropy=[-0.1427411], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0013244239380583167, KL divergence=[0.], Entropy=[-0.14275795], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003567380364984274, KL divergence=[0.], Entropy=[-0.14275396], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.006886680144816637, KL divergence=[0.], Entropy=[-0.14275567], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0019934887532144785, KL divergence=[0.], Entropy=[-0.14274418], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009340905584394932, KL divergence=[0.], Entropy=[-0.14274877], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.007974422536790371, KL divergence=[0.], Entropy=[-0.14274834], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0023918987717479467, KL divergence=[0.], Entropy=[-0.1427526], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/38_Step-80378.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=158, Total reward=-1.27, Steps=80884, Training iteration=39\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=159, Total reward=-0.81, Steps=81390, Training iteration=39\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=160, Total reward=-0.46, Steps=81896, Training iteration=39\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=160, Total reward=1.61, Steps=81920, Training iteration=39\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=160, Total reward=1.66, Steps=81920, Training iteration=39\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=160, Total reward=1.67, Steps=81920, Training iteration=39\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=160, Total reward=1.69, Steps=81920, Training iteration=39\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=160, Total reward=1.66, Steps=81920, Training iteration=39\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.66\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=161, Total reward=-1.11, Steps=82426, Training iteration=39\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0034631122834980488, KL divergence=[0.], Entropy=[-0.14277487], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.006442705634981394, KL divergence=[0.], Entropy=[-0.14278379], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0018585927318781614, KL divergence=[0.], Entropy=[-0.14278461], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00013654654321726412, KL divergence=[0.], Entropy=[-0.14279355], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003605655860155821, KL divergence=[0.], Entropy=[-0.1427903], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00048244287609122694, KL divergence=[0.], Entropy=[-0.1427845], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0021888883784413338, KL divergence=[0.], Entropy=[-0.14279185], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004089099355041981, KL divergence=[0.], Entropy=[-0.14278692], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00352843152359128, KL divergence=[0.], Entropy=[-0.1427898], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004370382055640221, KL divergence=[0.], Entropy=[-0.14279954], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/39_Step-82426.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=162, Total reward=-1.0, Steps=82932, Training iteration=40\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=163, Total reward=-1.09, Steps=83438, Training iteration=40\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=164, Total reward=-0.81, Steps=83944, Training iteration=40\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=164, Total reward=1.69, Steps=83968, Training iteration=40\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=164, Total reward=1.7, Steps=83968, Training iteration=40\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=164, Total reward=1.75, Steps=83968, Training iteration=40\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=164, Total reward=1.73, Steps=83968, Training iteration=40\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=164, Total reward=1.69, Steps=83968, Training iteration=40\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.71\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=165, Total reward=-0.72, Steps=84474, Training iteration=40\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0015688021667301655, KL divergence=[0.], Entropy=[-0.14281313], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.010398644022643566, KL divergence=[0.], Entropy=[-0.14283787], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0006094666314311326, KL divergence=[0.], Entropy=[-0.14284046], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.005268752109259367, KL divergence=[0.], Entropy=[-0.14284867], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004771461244672537, KL divergence=[0.], Entropy=[-0.142843], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0013277478283271194, KL divergence=[0.], Entropy=[-0.14283794], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001962475711479783, KL divergence=[0.], Entropy=[-0.14284863], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0021572166588157415, KL divergence=[0.], Entropy=[-0.14285132], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0010597753571346402, KL divergence=[0.], Entropy=[-0.14284554], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0005543401348404586, KL divergence=[0.], Entropy=[-0.14284308], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/40_Step-84474.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=166, Total reward=-0.61, Steps=84980, Training iteration=41\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=167, Total reward=-0.81, Steps=85486, Training iteration=41\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=168, Total reward=-0.78, Steps=85992, Training iteration=41\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=168, Total reward=1.6, Steps=86016, Training iteration=41\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=168, Total reward=1.64, Steps=86016, Training iteration=41\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=168, Total reward=1.61, Steps=86016, Training iteration=41\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=168, Total reward=1.64, Steps=86016, Training iteration=41\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=168, Total reward=1.66, Steps=86016, Training iteration=41\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.63\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=169, Total reward=-1.37, Steps=86522, Training iteration=41\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0007400012691505253, KL divergence=[0.], Entropy=[-0.14285807], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004943623673170805, KL divergence=[0.], Entropy=[-0.14288375], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0032371561974287033, KL divergence=[0.], Entropy=[-0.14287722], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0015969611704349518, KL divergence=[0.], Entropy=[-0.14287919], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002591555705294013, KL divergence=[0.], Entropy=[-0.14289406], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002886650152504444, KL divergence=[0.], Entropy=[-0.14288412], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.000143543365993537, KL divergence=[0.], Entropy=[-0.14288333], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0008934322977438569, KL divergence=[0.], Entropy=[-0.1428856], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0005046422593295574, KL divergence=[0.], Entropy=[-0.14287041], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0031421564053744078, KL divergence=[0.], Entropy=[-0.14287657], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/41_Step-86522.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=170, Total reward=-0.85, Steps=87028, Training iteration=42\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=171, Total reward=-0.78, Steps=87534, Training iteration=42\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=172, Total reward=-0.8, Steps=88040, Training iteration=42\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=172, Total reward=1.62, Steps=88064, Training iteration=42\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=172, Total reward=1.68, Steps=88064, Training iteration=42\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=172, Total reward=1.69, Steps=88064, Training iteration=42\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=172, Total reward=1.81, Steps=88064, Training iteration=42\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=172, Total reward=1.64, Steps=88064, Training iteration=42\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.69\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=173, Total reward=-1.36, Steps=88570, Training iteration=42\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002441669814288616, KL divergence=[0.], Entropy=[-0.14290002], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003737072227522731, KL divergence=[0.], Entropy=[-0.14289577], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0027953232638537884, KL divergence=[0.], Entropy=[-0.14290187], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0028077205643057823, KL divergence=[0.], Entropy=[-0.14291735], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002226704964414239, KL divergence=[0.], Entropy=[-0.14290933], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007358985487371683, KL divergence=[0.], Entropy=[-0.14291622], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.009063316509127617, KL divergence=[0.], Entropy=[-0.14291288], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.001958327367901802, KL divergence=[0.], Entropy=[-0.14291598], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00278331502340734, KL divergence=[0.], Entropy=[-0.14291994], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0010892864083871245, KL divergence=[0.], Entropy=[-0.1429232], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/42_Step-88570.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=174, Total reward=-1.07, Steps=89076, Training iteration=43\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=175, Total reward=-1.49, Steps=89582, Training iteration=43\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=176, Total reward=-0.87, Steps=90088, Training iteration=43\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=176, Total reward=1.56, Steps=90112, Training iteration=43\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=176, Total reward=1.67, Steps=90112, Training iteration=43\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=176, Total reward=1.74, Steps=90112, Training iteration=43\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=176, Total reward=1.78, Steps=90112, Training iteration=43\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=176, Total reward=1.67, Steps=90112, Training iteration=43\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.68\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=177, Total reward=-0.41, Steps=90618, Training iteration=43\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002776522422209382, KL divergence=[0.], Entropy=[-0.14294143], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.006699500139802694, KL divergence=[0.], Entropy=[-0.14296637], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00416982639580965, KL divergence=[0.], Entropy=[-0.14297104], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.007054389454424381, KL divergence=[0.], Entropy=[-0.14296629], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00046539402683265507, KL divergence=[0.], Entropy=[-0.14296718], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0004089587600901723, KL divergence=[0.], Entropy=[-0.14296214], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0029241787269711494, KL divergence=[0.], Entropy=[-0.14298442], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0019536244217306376, KL divergence=[0.], Entropy=[-0.14297114], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.006106690037995577, KL divergence=[0.], Entropy=[-0.14296907], training epoch=8, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=0.007456258404999971, KL divergence=[0.], Entropy=[-0.14298852], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/43_Step-90618.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=178, Total reward=-1.12, Steps=91124, Training iteration=44\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=179, Total reward=-0.85, Steps=91630, Training iteration=44\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=180, Total reward=-0.82, Steps=92136, Training iteration=44\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=180, Total reward=1.7, Steps=92160, Training iteration=44\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=180, Total reward=1.75, Steps=92160, Training iteration=44\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=180, Total reward=1.73, Steps=92160, Training iteration=44\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=180, Total reward=1.63, Steps=92160, Training iteration=44\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=180, Total reward=1.61, Steps=92160, Training iteration=44\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.69\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=181, Total reward=-0.89, Steps=92666, Training iteration=44\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0032194103114306927, KL divergence=[0.], Entropy=[-0.14301239], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002043984131887555, KL divergence=[0.], Entropy=[-0.14301397], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002879554871469736, KL divergence=[0.], Entropy=[-0.14300661], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004889595787972212, KL divergence=[0.], Entropy=[-0.1430217], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003066370962187648, KL divergence=[0.], Entropy=[-0.14302994], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0016377035062760115, KL divergence=[0.], Entropy=[-0.1430278], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0009783775312826037, KL divergence=[0.], Entropy=[-0.14301717], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0005911269108764827, KL divergence=[0.], Entropy=[-0.14301431], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0013576779747381806, KL divergence=[0.], Entropy=[-0.14302117], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.005473060999065638, KL divergence=[0.], Entropy=[-0.1430355], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/44_Step-92666.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=182, Total reward=-1.01, Steps=93172, Training iteration=45\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=183, Total reward=-0.97, Steps=93678, Training iteration=45\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=184, Total reward=-0.66, Steps=94184, Training iteration=45\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=184, Total reward=1.68, Steps=94208, Training iteration=45\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=184, Total reward=1.72, Steps=94208, Training iteration=45\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=184, Total reward=1.78, Steps=94208, Training iteration=45\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=184, Total reward=1.85, Steps=94208, Training iteration=45\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=184, Total reward=1.79, Steps=94208, Training iteration=45\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.76\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=185, Total reward=-0.85, Steps=94714, Training iteration=45\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0008308296091854572, KL divergence=[0.], Entropy=[-0.14305025], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.007300776895135641, KL divergence=[0.], Entropy=[-0.14306404], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0011024863924831152, KL divergence=[0.], Entropy=[-0.14305514], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0034456951543688774, KL divergence=[0.], Entropy=[-0.14306803], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0018412908539175987, KL divergence=[0.], Entropy=[-0.1430713], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.006063843611627817, KL divergence=[0.], Entropy=[-0.14305796], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002700487617403269, KL divergence=[0.], Entropy=[-0.14306183], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004353094846010208, KL divergence=[0.], Entropy=[-0.14306489], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002523216186091304, KL divergence=[0.], Entropy=[-0.14307006], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0014096233062446117, KL divergence=[0.], Entropy=[-0.143071], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/45_Step-94714.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=186, Total reward=-1.16, Steps=95220, Training iteration=46\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=187, Total reward=-1.1, Steps=95726, Training iteration=46\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=188, Total reward=-0.95, Steps=96232, Training iteration=46\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=188, Total reward=1.75, Steps=96256, Training iteration=46\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=188, Total reward=1.8, Steps=96256, Training iteration=46\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=188, Total reward=1.78, Steps=96256, Training iteration=46\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=188, Total reward=1.78, Steps=96256, Training iteration=46\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=188, Total reward=1.75, Steps=96256, Training iteration=46\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.77\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=189, Total reward=-0.72, Steps=96762, Training iteration=46\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001308596576564014, KL divergence=[0.], Entropy=[-0.14307126], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0007441344205290079, KL divergence=[0.], Entropy=[-0.14309499], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005486018490046263, KL divergence=[0.], Entropy=[-0.14310257], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.001043462660163641, KL divergence=[0.], Entropy=[-0.14308739], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0014592567458748817, KL divergence=[0.], Entropy=[-0.14310078], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.002819231478497386, KL divergence=[0.], Entropy=[-0.14311296], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.005946967285126448, KL divergence=[0.], Entropy=[-0.14310943], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.001366312731988728, KL divergence=[0.], Entropy=[-0.1431035], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.004382135812193155, KL divergence=[0.], Entropy=[-0.14311357], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.003634755266830325, KL divergence=[0.], Entropy=[-0.14312343], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/46_Step-96762.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=190, Total reward=-0.82, Steps=97268, Training iteration=47\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=191, Total reward=-1.11, Steps=97774, Training iteration=47\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=192, Total reward=-1.05, Steps=98280, Training iteration=47\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=192, Total reward=1.82, Steps=98304, Training iteration=47\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=192, Total reward=1.87, Steps=98304, Training iteration=47\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=192, Total reward=1.93, Steps=98304, Training iteration=47\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=192, Total reward=1.88, Steps=98304, Training iteration=47\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=192, Total reward=1.86, Steps=98304, Training iteration=47\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.87\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=193, Total reward=-0.56, Steps=98810, Training iteration=47\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=0.00681733526289463, KL divergence=[0.], Entropy=[-0.14313146], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0023761752527207136, KL divergence=[0.], Entropy=[-0.14313105], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0020642364397644997, KL divergence=[0.], Entropy=[-0.14314283], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0035560810938477516, KL divergence=[0.], Entropy=[-0.14313538], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001221068436279893, KL divergence=[0.], Entropy=[-0.14315224], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0014157358091324568, KL divergence=[0.], Entropy=[-0.14313838], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0037839969154447317, KL divergence=[0.], Entropy=[-0.14314829], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0012779502430930734, KL divergence=[0.], Entropy=[-0.1431509], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0019285654416307807, KL divergence=[0.], Entropy=[-0.14314775], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.001979261636734009, KL divergence=[0.], Entropy=[-0.14313903], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/47_Step-98810.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=194, Total reward=-0.86, Steps=99316, Training iteration=48\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=195, Total reward=-1.04, Steps=99822, Training iteration=48\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=196, Total reward=-1.32, Steps=100328, Training iteration=48\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=196, Total reward=1.93, Steps=100352, Training iteration=48\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=196, Total reward=1.78, Steps=100352, Training iteration=48\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=196, Total reward=1.77, Steps=100352, Training iteration=48\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=196, Total reward=1.86, Steps=100352, Training iteration=48\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=196, Total reward=1.93, Steps=100352, Training iteration=48\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.85\u001b[0m\n",
      "\u001b[34m2020-03-29 06:24:23,017 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-29 06:25:37 Uploading - Uploading generated training model\n",
      "2020-03-29 06:25:37 Completed - Training job completed\n",
      "Training seconds: 648\n",
      "Billable seconds: 648\n",
      "\n",
      "Job name: drl-portfolio-optimization-2020-03-29-06-12-09-827\n",
      "\n",
      "Waiting for s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-29-06-12-09-827/output/output.tar.gz...\n",
      "Downloading drl-portfolio-optimization-2020-03-29-06-12-09-827/output/output.tar.gz\n",
      "\n",
      "Copied output files to 'tmp_dir': /tmp/drl-portfolio-optimization-2020-03-29-06-12-09-827\n",
      "\n",
      "Waiting for s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-29-06-12-09-827/output/intermediate/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv...\n",
      "Downloading drl-portfolio-optimization-2020-03-29-06-12-09-827/output/intermediate/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First define the Reinforcement Learning Estimator\n",
    "job_name_prefix = 'drl-portfolio-optimization'\n",
    "estimator = RLEstimator(source_dir='src',\n",
    "                        entry_point=\"train-coach.py\",\n",
    "                        dependencies=[\"common/sagemaker_rl\"],\n",
    "                        toolkit=RLToolkit.COACH,\n",
    "                        toolkit_version='0.11.0',\n",
    "                        framework=RLFramework.MXNET,\n",
    "                        role=role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type=instance_type,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        hyperparameters = {\n",
    "                            \"RLCOACH_PRESET\" : \"preset-portfolio-management-clippedppo\",\n",
    "                            \"rl.agent_params.algorithm.discount\": 0.9,\n",
    "                            \"rl.evaluation_steps:EnvironmentEpisodes\": 5,\n",
    "                            \"training_epochs\": 10,\n",
    "                            \"improve_steps\":100000})\n",
    "\n",
    "# Perform the training\n",
    "if rerun:\n",
    "    estimator.fit({'checkpoint': checkpoint_path})\n",
    "else:\n",
    "    estimator.fit()\n",
    "    \n",
    "# Bring the training output back to the Sagemaker instance\n",
    "job_name=estimator._current_job_name\n",
    "print(\"\\nJob name: {}\\n\".format(job_name))\n",
    "\n",
    "output_tar_key = \"{}/output/output.tar.gz\".format(job_name)\n",
    "intermediate_folder_key = \"{}/output/intermediate/\".format(job_name)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "local_checkpoint_path = tmp_dir + '/checkpoint'\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "wait_for_s3_object(s3_bucket, output_tar_key, tmp_dir)  \n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "os.system(\"aws s3 cp --recursive {} {}\".format(intermediate_url, tmp_dir))\n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "print(\"\\nCopied output files to 'tmp_dir': {}\\n\".format(tmp_dir))\n",
    "\n",
    "\n",
    "# Plot the training history\n",
    "csv_file_name = \"worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\"\n",
    "wait_for_s3_object(s3_bucket, os.path.join(intermediate_folder_key, csv_file_name), tmp_dir)\n",
    "\n",
    "df = pd.read_csv(\"{}/{}\".format(tmp_dir, csv_file_name))\n",
    "df = df.dropna(subset=['Evaluation Reward'])\n",
    "x_axis = 'Episode #'\n",
    "y_axis = 'Evaluation Reward'\n",
    "\n",
    "plt = df.plot(x=x_axis,y=y_axis, figsize=(12,5), legend=True, style='b-')\n",
    "plt.set_ylabel(y_axis);\n",
    "plt.set_xlabel(x_axis);\n",
    "\n",
    "rerun = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Policy through the Test Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evaluation on the test data\n",
    "Note the major differences from the training step above is the 2 year `evaluate_steps` hyperparameter, the `evaluate-coach.py` vs `train-coach.py` entry point and the `checkpoint` argument passed to the fit function.  At the start of the entry point script, the training or test dataset are selected ensuring there is no data leakage from the training into the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  /tmp/drl-portfolio-optimization-2020-03-29-06-12-09-827/checkpoint\n",
      "dst:  s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-29-06-12-09-827/checkpoint/\n"
     ]
    }
   ],
   "source": [
    "local_checkpoint_path = tmp_dir + '/checkpoint'\n",
    "s3_checkpoint_path = \"s3://{}/{}/checkpoint/\".format(s3_bucket, job_name)\n",
    "os.system(\"aws s3 cp --recursive {} {}\".format(local_checkpoint_path, s3_checkpoint_path))\n",
    "print('src:  ' + local_checkpoint_path)\n",
    "print('dst:  ' + s3_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-29 06:26:02 Starting - Starting the training job...\n",
      "2020-03-29 06:26:05 Starting - Launching requested ML instances............\n",
      "2020-03-29 06:28:15 Starting - Preparing the instances for training...\n",
      "2020-03-29 06:28:57 Downloading - Downloading input data...\n",
      "2020-03-29 06:29:31 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:32,650 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:32,653 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:32,665 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HPS': '{\"evaluate_steps\":506}', 'SM_USER_ENTRY_POINT': 'evaluate-coach.py', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-29-06-26-01-782/source/sourcedir.tar.gz', 'SM_INPUT_DATA_CONFIG': '{\"checkpoint\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNELS': '[\"checkpoint\"]', 'SM_NUM_CPUS': '8', 'SM_HP_EVALUATE_STEPS': '506', 'SM_CHANNEL_CHECKPOINT': '/opt/ml/input/data/checkpoint', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_estimator\":\"RLEstimator\"}', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{\"checkpoint\":\"/opt/ml/input/data/checkpoint\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"evaluate_steps\":506},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"checkpoint\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-rl-mxnet-2020-03-29-06-26-01-782\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-29-06-26-01-782/source/sourcedir.tar.gz\",\"module_name\":\"evaluate-coach\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"evaluate-coach.py\"}', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_CURRENT_HOST': 'algo-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_MODULE_NAME': 'evaluate-coach', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_USER_ARGS': '[\"--evaluate_steps\",\"506\"]', 'SM_HOSTS': '[\"algo-1\"]', 'SM_NUM_GPUS': '0', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_LOG_LEVEL': '20', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_INPUT_DIR': '/opt/ml/input'}\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:32,830 sagemaker-containers INFO     Module evaluate-coach does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:32,830 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:32,830 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:32,830 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: evaluate-coach\n",
      "  Running setup.py bdist_wheel for evaluate-coach: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for evaluate-coach: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-65kpq43b/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built evaluate-coach\u001b[0m\n",
      "\u001b[34mInstalling collected packages: evaluate-coach\u001b[0m\n",
      "\u001b[34mSuccessfully installed evaluate-coach-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:34,516 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:34,528 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"job_name\": \"sagemaker-rl-mxnet-2020-03-29-06-26-01-782\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"module_name\": \"evaluate-coach\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"user_entry_point\": \"evaluate-coach.py\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"log_level\": 20,\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator\": \"RLEstimator\"\n",
      "    },\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"checkpoint\": \"/opt/ml/input/data/checkpoint\"\n",
      "    },\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-29-06-26-01-782/source/sourcedir.tar.gz\",\n",
      "    \"input_data_config\": {\n",
      "        \"checkpoint\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"hyperparameters\": {\n",
      "        \"evaluate_steps\": 506\n",
      "    },\n",
      "    \"network_interface_name\": \"eth0\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HPS={\"evaluate_steps\":506}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=evaluate-coach.py\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=evaluate-coach\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"checkpoint\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"checkpoint\"]\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATE_STEPS=506\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CHECKPOINT=/opt/ml/input/data/checkpoint\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator\":\"RLEstimator\"}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{\"checkpoint\":\"/opt/ml/input/data/checkpoint\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"evaluate_steps\":506},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"checkpoint\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-rl-mxnet-2020-03-29-06-26-01-782\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-29-06-26-01-782/source/sourcedir.tar.gz\",\"module_name\":\"evaluate-coach\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"evaluate-coach.py\"}\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-29-06-26-01-782/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--evaluate_steps\",\"506\"]\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m evaluate-coach --evaluate_steps 506\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#033[93mWarning: failed to import the following packages - tensorflow#033[0m\u001b[0m\n",
      "\u001b[34mLoading preset preset-portfolio-management-clippedppo from /opt/ml/code\u001b[0m\n",
      "\u001b[34m## Creating graph - name: BasicRLGraphManager\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\u001b[0m\n",
      "\u001b[34m## Creating agent - name: agent\u001b[0m\n",
      "\u001b[34mRequested devices [gpu(0)] not available. Default to CPU context.\u001b[0m\n",
      "\u001b[34mRequested devices [gpu(0)] not available. Default to CPU context.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m## Loading checkpoint: /opt/ml/input/data/checkpoint/47_Step-98810.ckpt\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=1, Total reward=1.89, Steps=506, Training iteration=0\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1.89\u001b[0m\n",
      "\u001b[34m2020-03-29 06:29:38,172 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-29 06:29:49 Uploading - Uploading generated training model\n",
      "2020-03-29 06:29:49 Completed - Training job completed\n",
      "Training seconds: 52\n",
      "Billable seconds: 52\n",
      "Waiting for s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-29-06-26-01-782/output/output.tar.gz...\n",
      "Downloading sagemaker-rl-mxnet-2020-03-29-06-26-01-782/output/output.tar.gz\n",
      "\n",
      "Copied output files to /tmp/sagemaker-rl-mxnet-2020-03-29-06-26-01-782\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0e37943f98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4VFX6wPHvSe+EFEJPCC30FpoUKYroqlhAsKx1RVdl7QVde9n9qeuqa1+7oqLYERR16b2FXhIgJCEhlfQ6M+f3x5k0CCRgJjNJ3s/z5Elm5sy970zuvHPuaVdprRFCCNHyuTk7ACGEEE1DEr4QQrQSkvCFEKKVkIQvhBCthCR8IYRoJSThCyFEKyEJXwghWglJ+EII0Uo4LOErpXorpeJq/OQrpe5y1P6EEEKcmmqKmbZKKXfgCDBSa33Y4TsUQghxAo8m2s9k4EB9yT4sLExHRUU1TURCCNFCbN68OUtrHV5fuaZK+LOAz+srFBUVxaZNm5ogHCGEaDmUUg1qOXF4p61Sygu4GPjqJI/PVkptUkptyszMdHQ4QgjRajXFKJ3zgS1a6/S6HtRav6O1jtVax4aH13tGIoQQ4gw1RcK/kgY05wghhHAsh7bhK6X8gXOBW850GxUVFaSkpFBaWtp4gQmH8vHxoXPnznh6ejo7FCFEDQ5N+FrrIiD0j2wjJSWFwMBAoqKiUEo1UmTCUbTWZGdnk5KSQrdu3ZwdjhCiBpefaVtaWkpoaKgk+2ZCKUVoaKickQlRj7ziClJzS5p0ny6f8AFJ9s2M/L+EOLX1B7OZ/NJyrvrvuibdb7NI+EII0VJYbZq/fLSJrMIyjuSWoLUmIaOQBxZsw2K1OXTfkvCbwIcffkhqamrV7ZUrV9KvXz8GDx5MSUndp3SJiYn0798fgE2bNvG3v/2tSeK84447HL4fIVoTrTUfrD7Ej9tMDkjNLaGgzEJ0uD8VVk1RuZUFm1P4clMKRxzcxCMJ38GsVusJCX/evHnMnTuXuLg4fH19691GbGwsr776qiPDFEI4SHJOCU/+uJs5n28F4HB2MQCDuwQD8MbSBDYm5gCQX2Kpel5JubXRY2mqpRUaxZM/7mJ3an6jbrNvxyAev6jfKcskJiYydepUhg0bxpYtW+jXrx8ff/wxa9eu5b777sNisTB8+HDefPNNvL29iYqKYubMmfz666/cc889bNq0iauvvhpfX19uuukmvvzyS3755RcWL17Mp59+ygMPPMDixYtRSvH3v/+dmTNn1tr/smXLePHFF1m4cCE5OTnceOONHDx4ED8/P9555x0GDhx4Qsw2m43o6Gji4uIIDjYHVs+ePVm1ahUbNmzgmWeeoby8nNDQUObNm0dERESt519//fVceOGFTJ8+HYCAgAAKCwsBeOGFF/jyyy8pKyvj0ksv5cknnzzj91+Ili41r7rWnlNUTmJ2EWAS/jdbjvDGsgNVj+eXVgCw5kAWV/13PV/dOprhUSEn3XZecQWP/bCzwbFIDb+B9u3bx2233caePXsICgripZde4vrrr2f+/Pns2LEDi8XCm2++WVU+NDSULVu2cM011xAbG8u8efOIi4tjzpw5XHzxxbzwwgvMmzePb775hri4OLZt28Zvv/3G/fffT1pa2knjePzxxxkyZAjbt2/nueee49prr62znJubG9OmTePbb78FYP369URGRhIREcHYsWNZt24dW7duZdasWTz//PMNfh+WLFlCfHw8GzZsIC4ujs2bN7NixYoGP1+I1uZoXvWItZ1H8pi/MZlAHw/6dgg6oWx+iUn4W5NyAVi048RcoLWuqv2vOZDF93GpJ5Q5mWZVw6+vJu5IXbp0YcyYMQBcc801PP3003Tr1o1evXoBcN111/H6669z111myf/ja+kns2rVKq688krc3d2JiIjg7LPPZuPGjXXW2ivLf/311wBMmjSJ7Oxs8vPzCQo68eCZOXMmTz31FDfccANffPFFVUwpKSnMnDmTtLQ0ysvLT2u8/JIlS1iyZAlDhgwBoLCwkPj4eMaPH9/gbQjRWlRYbby13NTgvT3cuPb9DQC09fMkIsjnhPJ59oTv6+kOQFruicOb31+dyNMLd7PxkXNYlZBFgHfD07jU8Bvo+KGGlc0kJ+Pv7+/IcBpk9OjRJCQkkJmZyXfffcdll10GwJw5c7jjjjvYsWMHb7/9dp1j5j08PLDZzIgBm81GeXk5YGoXlf0PcXFxJCQkcNNNNzXdixKiGcgrqSA+vYALX13F3qMFdAr25Yc7xuLtYVLuU9P60yXEj5mxXWo9r7JJ51hxea3fNf0QdwSAxOwijuaVEhnq1+C4JOE3UFJSEmvXrgXgs88+IzY2lsTERBISEgD45JNPOPvss+t8bmBgIAUFBXU+Nm7cOObPn4/VaiUzM5MVK1YwYsSIk8Yxbtw45s2bB5i2/bCwsDpr92C+pC699FLuuece+vTpQ2iomfScl5dHp06dAPjoo4/qfG5UVBSbN28G4IcffqCiwhyI5513Hu+//35Ve/6RI0fIyMg4abxCtDZrD2Qz6MklnPvvFRzKKuLVK4fw813j6N0+kK2Pncvep6dy0aCOAPzf9IFMjmnH5Jh2uLupqk7b7CKT6DMKytiRkkdxeXVnrp+XqdEfKyqnoNRCoE/Da/jNqknHmXr37s3rr7/OjTfeSN++fXn11VcZNWoUM2bMqOq0vfXWW+t87vXXX8+tt96Kr69v1ZdGpUsvvZS1a9cyaNAglFI8//zztG/fnsTExDq39cQTT3DjjTcycOBA/Pz8TpqwK82cOZPhw4fz4Ycf1trGjBkzaNu2LZMmTeLQoUMnPO/mm29m2rRpDBo0iKlTp1adsUyZMoU9e/YwevRowHTmfvrpp7Rr1+6UcQjRGqTllXDHZ1uqbp/VI5SL7ckdqpN1Te9dPxyAEc/+xtF8c7Z9zJ7wD2cXcdFrq/jTwA68ftVQAPy9TXPP0fxS8ksr6BLS8Bp+k1zisKFiY2P18RdA2bNnD3369HFSREZiYiIXXnghO3c2vDe8tXOF/5sQjpZfWkGQj1kk0GbTTH9rDfuOFnBWjzB+3Z3O/ef15vaJPRq0rVnvrKXcYmP+LaOZ8MKyWmPyPdwUe56eiqe7G7d+spmfdx3l+rOi+HV3OiOjQ/j3zCGbtdax9e1DmnSEEOI0bE06RnJOMXvS8hn4xBK+t7ep7ziSx5akXOZe0IdnLunPpUM68efRkQ3ebnR4AAcyi9iWnMuR3BLmTOrB2B5h/P1PfbDYNLvsQ9ILykzz6rdbj3CsuJzA0+i0lSadBoiKinLp2v0HH3zAK6+8Uuu+MWPG8PrrrzspIiFapnKLjUvfWAPADWOiAHh35SHO69eemz82rRNT+7cnLMCbf88cfFrb7tzWl7ySCnYeyQNg+rDO3DulN2l5JTzz0x62JecyuEsw+SUWgv08yS02iT/Qp+HLkEvCbwFuuOEGbrjhBmeHIUSLtys1r+rvD9ckAqZmP+fzrWQUlPHwBTGEBXif0bYrm4a2peTh5e5G57ambT7cvr2conJKyq2kHCtmUkw7knOK2Zh47LQ6baVJRwghGmDz4Rye+WlP1e0Abw+evsSsd1XZXn/zuOgz3n5l4t6WnEtkqB/ubmYouIe7GwHeHny8NpEFW1I4VlzB9GGduWV8dwCC/aSGL4QQjWbpvgxu+GAjwX6e/GvGIH7cnsr0YZ05v38H2vp50rNdIL3bB/6hfQT5msR9MKuI8/rVXuqksMwMy3zyh114ebgxIioEdzfFK7MGMzGm4SPkJOELIcQpaK15acl+uoX5s3DOWPy9Pbh8WOeqxy8c2PEUz264oBpNM9HhAXWWsdg0/TsE4uFuGmemDe50WvuQhC+EEKdwOLuYHUfyePyivvifxoiY01Wz8zU6rO6Z+g+dH0PviDM/k5A2/Ca2bNkyLrzwwgaXj4uLY9GiRY0ag6x7L0TDHcoyq1sO7NzGofsJqpnwT1LDv3lc9Gk14RzPoQlfKRWslFqglNqrlNqjlBrtyP25OovFUn+h4zgi4QshGq5yOePIUMeujxXkW3320D289r5GdjNLJFd25J4pRzfpvAL8rLWerpTyAho+B7guix+CozsaJbAq7QfA+f88ZZHK9fBHjRrFmjVrGD58ODfccAOPP/44GRkZVWvb3HnnnZSWluLr68sHH3xA7969+fDDD/nmm28oLCzEarXWWjt+48aNzJ49mwULFtC+fXvmzJnDzp07qaio4IknnuD888/nscceo6SkhFWrVjF37twTVuGUde+FcIyScitrDmRxILOQQG8PQv29HLo/Py8PzukTwcGsQoL9au/r45tGUG7545c/dFgNXynVBhgPvAegtS7XWuc6an+OlpCQwL333svevXvZu3cvn332GatWreLFF1/kueeeIyYmhpUrV7J161aeeuopHn744arnbtmyhQULFrB8+fKq+9asWcOtt97K999/T/fu3Xn22WeZNGkSGzZsYOnSpdx///1UVFTw1FNPMXPmTOLi4upcclnWvRfCMeatP8xNH23is/VJDOoSfMKKuY7w32uHseSuE5ca9/ZwP60JVifjyBp+NyAT+EApNQjYDNyptS464y3WUxN3pG7dujFgwAAA+vXrx+TJk1FKMWDAABITE8nLy+O6664jPj4epVTV6pIA5557LiEh1Vet2bNnD7Nnz2bJkiV07Gh6+JcsWcIPP/zAiy++CEBpaSlJSUkNik3WvRei8e1PL8DTXVFh1ZzdK7xJ9qmUwsPdcV8sjmzD9wCGAm9qrYcARcBDxxdSSs1WSm1SSm3KzMx0YDh/jLd39ew5Nze3qttubm5YLBYeffRRJk6cyM6dO/nxxx9rrTF//Nr4HTp0wMfHh61bt1bdp7Xm66+/rlpnPikpqcGLj8m690I0rsSsIr7clEK/jm1Ycf9Ebhzb8MqSK3Nkwk8BUrTW6+23F2C+AGrRWr+jtY7VWseGhzfNt6gj1FxjvuZSxHUJDg7mp59+Yu7cuSxbtgww68z/5z//oXL10sovg1OtpV9J1r0XovHkFVdUrYszKaYdXWvMem3uHJbwtdZHgWSlVG/7XZOB3Y7an7M98MADzJ07lyFDhjRoNE5ERAQLFy7k9ttvZ/369Tz66KNUVFQwcOBA+vXrx6OPPgrAxIkT2b17N4MHD2b+/Pkn3d7MmTP59NNPa7XzV657P2zYMMLCwup83s0338zy5csZNGgQa9eurbXu/VVXXcXo0aMZMGAA06dPr/eLRwhnWncwu+qKUX/ED9tTic8o5D9XDuFvk3s2QmSuw6Hr4SulBgPvAl7AQeAGrfWxk5V31fXwxemT/5toCvHpBfyy6yiju4dy+ZtruXlcNx75U9/T2sbTC3fT1s+TOyaZ5H7jhxtJyChk+f0TmqSjtjEopRq0Hr5Dh2VqreOAeoMQQogzcdkbaygosxAWYIYxLtpxlIcv6NPgRF1aYeWTtYcpt9roHh7AxJh2rDmQxczYLs0m2Z8OmWnbjHzwwQcMHjy41s/tt9/u7LCEaFL70wv4dmsKFquNAvuiYlmF5QyPasuR3BLikhs++nt7Sh7lVhvBfp488PV2lu7NoLTC9odms7qyZrGWjta6RX7bnq7msu69K102U7Qsu1Lz+NOrqwB4+bf4qvsjQ/149cohjP7H/9iSlEuQryf/WrKPl64YjI+n+0m3dzDTDEq4fUIPnl20h7/O24K7m2JUdKhjX4iTuHwN38fHh+zsbEkizYTWmuzsbHx8fJwdimiBFu84CsAVsZ3xdHejX8cgXrpiEG9cPZT2QT74ebmTcqyYe+bHsWjH0arLAp5M8rFi3N1UrRr94xf1PeWXRHPm8jX8zp07k5KSgiuP0Re1+fj40Llz5/oLCnEKdZ3ZJ2YX0TXEj+enD6rzOZ3b+vLbnnSSc0qqtnEqyTkldGjjQ3SYP6OjQ7lqZFcuGtQ4yx27IpdP+J6enqc1Q1QI0fwdzStlxttr6NDGl7nnxzCka1sAknKKiQw9+ZJcXdr68fve6vkipxqmabNptiYfo3t4AG5uis9nj2q8F+CiXL5JRwjR+ny8NpHknBLiknK59I013PHZFvanF7A3reCU68HfO6U3/7lyCGN6mDb4/JKTz4n5yL6P6cNaz9moJHwhhMv5394MRkeHsuWxc/nrhO4s3J7GFW+vxd1N8ZdTXDe2b8cgLhrUkVdmmTWgKmv4ucXltcptSTrGc4v2cE6fdvxpQAfHvRAXIwlfCOFUWmv+u+IgUQ/9xHn/XkFhmYV96QWMig4lwNuD+6f0xt/LndziCm6b0J32beofEFB5QfD8kgo+WXeYwU/9yrJ9pqmnzGJlzmdbad/Gh3/NGIxbC1k2oSFcvg1fCNGyPf/LPt5cdgCAfekFvPp7PFrDwC7mClNubopBXYI5nF3MzeNPXruvydvDHW8PN/JLLfy8y4zsOZxdDMDyfZkcyS3h/etjaeP3x5ccbk4k4QshnEZrzSdrDzMpph3/s3e2vrPiIKOjQxldYyz8y7MGozWnNVwyxN+LrMIysgtNc052UTk2m+anHWl4ebgxrmfzXazxTEnCF0I4TX6phcIyC6OjQ3n32ljeXXWQEH9vLhvSqVZTS7vA05/X0SXEj23JuaTlmWXBswvLeOX3eL6PSyXA2wNP99bXoi0JXwjhFKsTslh7IBuATm19cXNTzB7fvdG2Hxnix4ZDOfh6ulNutbHhUA7lVnPth8Ky07++dEsgCV8I0eR+2XWUWz7ZXHW7Y7Bvo+8jOjwAgH9ePoDnFu0hPqOw6jGvVli7B0n4QoiTcNQaVnnFFcz5fCvRYf7EdAhEKUVM+5OPrT9T14zqytCuwYyMDsXdTbFkVzrF5VaSc4r5x+UDGn1/zYEkfCHECb7dmsLd87ex9dFz2X4kj+FRbfHz+uPpIiO/lKcW7qbcYmPuBX04t29EI0Rbt0AfT0baO34vHNiRCwe23CUTGqp1ntcIIU7p8/XJAFz+1hque38Dd34RV+9z0vJK2JNmFivLKiyrs8xHaxNZuD0NMOveiKYlCV8IcYLK8ekHM4sA2HAop96FyEb/43+c/8pKPlufROwzv/GXjzZisdpYnZDFjLfWkJhVRHx6dTt6J0n4TU4SvhCiSkFpBe+sOMCvu9MB8HBT3DahO3klFWw+fNKrk9by4pJ9APy2J4MXl+zn6nfXszHxGK8tTWBfevV1kYN8WtekJ1cgCV8IUeXmjzfx3KK9ANx7bi92Pnke5/VrD8BTC3ef9HmHsoqq/s4pKufZS/sD8NbyA1X3f7MlhcPZxTx36QD2Pj3VEeGLekjCF6KFyygo5a4vtpJXcvKlggESMgpZdzCHB6fGcOC5C7htYg98PN0Z1CWYy4Z0YndqPkUnGb/+zoqDVX8H+ngwY1iXqttr507ilVmDsWm4bUJ3rhzRpcVeYMTVOXSUjlIqESgArIClIVdVF0I0rv/8nsB3cakM7hLMdWdFoTV1Lhi280geAJP7tMP9uMevGtmVb7YeYf7GZG4cW/v6FBn5pXy9OYXhUW3ZmHiM2yb0wMvDjR/vGEu7IG8igny4eFBH+nYIoke7ALlcqRM1xbDMiVrrrCbYjxDCzmbT3PfVNmaN6MrhHLNo2OakXHanbWdlfBarH5yEm5uitMJaVdven16Ah5siKtT/hO3FRoUwOjqUN5Yd4MoRXfH1qq6hv7f6EBabjRdnDMKmIcp+gZIBndtUlVFK0fMU69iLpiFNOkK0QJmFZXyz9QhXvL2WTYk5APy4LZUvN6WQlldK8rFiFu9Io+9jP7Niv7l86I4jeXQPD8DLo+60cO+UXmQVlvHx2sSq+yxWG5+vT+L8AR2IDPWnW5i/1OBdmKMTvgaWKKU2K6VmO3hfQgi71NySqr+Ly628PHMwj1zQh3vP7QXArtR8XvhlHzYNX21OobTCyoZDOYzpEXbSbcZGhTC+VzjvrDhYNURzV2o++aUWpto7doVrc3STzlit9RGlVDvgV6XUXq31ipoF7F8EswG6du3q4HCEaB1Sc80Kkf+5cgh9OwYRba95l1ZYefn3eF5fmsBB+8ia3OJyEjIKKbPYGBbZ9pTbHd8zjBX7MzlWXEGIvxfb7e3+9T1PuAaH1vC11kfsvzOAb4ERdZR5R2sdq7WODQ9vfetTC+EIlTX88T3D6R5e3VHq4+lOj/AAdqXm07GND2N7hJFbXEGyvZ3/VBcIB7PGPMDQp38FIDO/FDcFEUGnv3yxaHoOS/hKKX+lVGDl38AUYKej9idEa2Ox2nh35UHiknNPeOxgVhHBfp4E+Z54Ej/Q3pl68/howgO9yS0pJ8me8LvWk/Db2hM+wPaUXF79XwI2zQmjeoRrcmQNPwJYpZTaBmwAftJa/+zA/QnRqvy6O51nftrD7fO2YLPVXvYgPr2AXu0C6+xAffzifnwxexTXjo6ija8nuUUVLN+fSXigd72zX0NrJPyHvt7ROC9ENBmHJXyt9UGt9SD7Tz+t9bOO2pcQrVHlAmVHckv4x+I9ACzdl8EVb69lS9IxekYE1Pm8AG8PRtmXDG7r50VBmYU1B7KZM6lHvfsMqZHw4zMKTlFSuCJZHlmIZqqwzArAZUM68e6qQ0zuE8HD3+xAAbec3Z0/j4qsdxvtgrwBiGkfyNUj6y8fFuBd9ffIbqGk55fy4NSYM3sBoslJwheimSoqs+Cm4JlL+7Ml6Riz3lkHwNOX9G9Qsge4fGhnfD3diY1q26B2eB9Pdw794wIAGW/fDMnEKyGaqaJyC/7eHvh5efCvKwYBZnXLKadxUREvDzcuGdKJzm1P3Vlbk1JKkn0zJTV8IZqpojIL/varUA2LDOHnu8ZhsWoZIilOShK+EM1UUZkVf+/qNW1i2gc5MRrRHEiTjhDNVFG5hQBvqbOJhpOEL0QzU1ph5dmfdpOQUdgoFxYXrYckfCFckMVq44Vf9jLz7bWUVlgpKK2oWrBs+f5M/rvyECnHSggJ8KpnS0JUk+qBEC7o+g82sirBXEZixltr2XEkj2mDO/LyzMGsO5iNj6cbH94wot61b4SoSRK+EC4mt7i8KtmDWace4Pu4VDzd3ViwOYWxPcIYFR3qrBBFMyVNOkK4mNUJ2VV/D4+qXna4b4cgFmxOAWDm8C4nPE+I+kgNXwgX88XGJNoFevPQ+TGc3Suc9Ydy8PZwIyzAm2mvr+bKEV24aFBHZ4cpmiFJ+EK4CK01v+5OZ2V8Fvef15vLhnYG4IIBHarK/HLXeLqGSLu9ODOS8IVwEesP5TD7k80AXD2y7qu/9W4vFwIXZ04SvhBOZLHaeGHJPpJzilm04ygAt03oTrCfDLcUjU8SvhBO9PWWFN5efpA2vtUXHnlAlhsWDiIJXwgn2p6SRxtfT7Y9PoXbP9uCh1wqUDiQJHwhnOhwdjFRYf4AvH7VUCdHI1o6GYcvhBMUllm464utrErIIlJG3Ygm4vAavlLKHdgEHNFaX+jo/Qnh6korrFz97nq2JecCMLBzGydHJFqLpqjh3wnsaYL9CNEsbEk6xrbkXGYM60y3MH8uHdLJ2SGJVsKhCV8p1Rn4E/CuI/cjRHOSWVAGmAuNL71vAqE1LgwuhCM5uob/MvAAYHPwfoRoNioTfnigJHrRtByW8JVSFwIZWuvN9ZSbrZTapJTalJmZ6ahwhHAZmQVleHm4EeQjg+RE03JkDX8McLFSKhH4ApiklPr0+EJa63e01rFa69jw8HAHhiOE8xWUVvD2ioNorVFKxtyLpuWwhK+1nqu17qy1jgJmAf/TWl/jqP0J0RzsTy8EYFhk23pKCtH4ZBy+EE0oLa8EgCcu7ufkSERr1CSNiFrrZcCyptiXEK4sLbcUgA5tfJ0ciWiNpIYvRBM6nFOEv5e7dNgKp5CEL0QTKbNYWbzjKKO7h0mHrXAKSfii1ckpKuf+r7aRV1zRpPv9ZVc62UXlXDOq7oubCOFokvBFq7PuYDZfbU7hu7gjDSq/PSWXQ1lFf3i/89YdpkuIL+N7yvBj4RyS8EWrYrNpPt+QBMCiHWn1lrfaNBe/tprJ/1p2WvupsNpqnUHEpxew/lAOV42IxE3WvBdOIj1HolV5Y1kCK+OzANiQmENGQSntAn1qlam87OBlQzqTUWBG1dg0dU6Wsto0K+IzGR4VQoC3B+sPZvPt1iN8sTEZgK9uHY2vpztrDph9Th/W2dEvUYiTkoQvWo2UY8W8uGR/1W2tTbv6n0dF8vbyA+QUl/PgeTF8su4wby8/iLe7G6sPZFeVv/6DjXi6m5Niq83Gkxf355ddR3l20R5uPbs7f5vcg798tAmLTVc9Z8ZbawHoFRFAsJ+nrJ8jnEoSvmg1vo9LrXW7e7g/i7ancc3Irvxj8V4AFu84WlWr/2pzCml5pTx5cT+OFZczb31S1cJnAPlfxhHqby42vnhnGsMi21JQZuHTm0byzdYUuocH0CsikFs+2cT+9EL6dwpqolcqRN0k4YtW44e4VIZFtsXd3oY+PKotby47wK7U/KoySTnFAIQFeJGWV0qnYF+uGtkVT3c3bpvQg4XbU3np1/0UlVnYfPhY1fMOZxez2N4nMKBTG8b2DKt6LDYqhA2HcvD1dG+KlynESUnCF63C3qP57Esv4Klp/bh2dBQACzanYNPw3dbq0TrjeoZx0cCOzN+UTFZhOef2jahqxvHycOOyoZ25bGhnrDbNzR9vYn96ATeO6cbTP+3mm61HaOvnSRs/z1r7/vCG4Xy4JpGhXWX9HOFckvBFq/B9XCrubooLBnSoui80wDTHvLvqEJ2CfVn14MSqTtmHv90BwKSYdnVuz91N8f71w6tu/7zrKBsO5RAZ6n9CWT8vD26b0KPRXosQZ0qGZYoWb2NiDm8uO8CYHmGE1bi6VJh/9d99OgTWGoFz+8QeKAVjeoTREGf3MmPre7YLaKSohWh8UsMXLdb7qw4Rn1FI+yAz7PKuc3rWeryyhg9w49hutR67+9xe3HVOzwYvgXDliK7EJedy53H7EMKVSMIXLdbXW1KqOmTDArxPaEMPsY+w8fF046zuJ9bkT2e9mxB/L/57bewfiFYIx5OEL1okq02TkFFIoI8HBaUWsgrLTigjhEnKAAAgAElEQVTj4+nOM5f0Z1R0qBMiFKLpSRu+aJGScoops9i465xeAAQfN3Km0jWjIukh7e6ilZAavmiR9qcXADC0azDz/jKSiCCZ4SqEJHzR4lhtmls+2QxAz4hAArzlMBcCpElHtEDZNdrrJdkLUU0+DaJFWbQjDZs2i5e9PHOwk6MRwrU4LOErpXyAFYC3fT8LtNaPO2p/QpSUW7lt3paq20G+Up8RoiZHfiLKgEla60KllCewSim1WGu9zoH7FK1YzcXMAAK86x6ZI0Rr5bCEr7XWQKH9pqf9R5/8GUL8MbvT8mrdlvZ7IWpz6CdCKeUObAZ6AK9rrdc7cn+i9dmTls8Hqw9xNL+MFfszaz0W6CMJX4iaHDpKR2tt1VoPBjoDI5RS/Y8vo5SarZTapJTalJmZeeJGhDiFzzcksWBzCttTcgEYERVSleilhi9EbU0yLFNrnQssBabW8dg7WutYrXVseHh4U4QjWpCCUgud2vpyzchIALq3C6haM8dfEr4QtThylE44UKG1zlVK+QLnAv/nqP2J1qmg1EKAtycDOrcBYHJMO7KLyjhWXI6Xh0wzEaImR1aBOgAf2dvx3YAvtdYLHbg/0QoVlVkI9PbgvH7tWXrfBLqFmQuQzBze1cmRCeF6HDlKZzswxFHbFwKgsMxCeKBZJ6cy2Qsh6ibnvKJZKyyzSOesEA0kCV80OzabZsOhHPKKKygotUjnrBANJJ8U0ex8uv4wj32/CzcFNi3j7YVoKKnhi2bn+7hUekcE0tbPXKIwUGr4QjSIJHzR7GQUlBLTIbBqnY5+nYKcGo8QzYUkfNHs5BSWE+LvxXn9IgCIjQpxckRCNA9yLiyahYz8Utr4eaI1FJVbCfX34ubx0dw2oQdBPrIqphANITV84fKsNs2I537njs+2klNUDkCIvzfeHu50CfFzcnRCNB+S8IXLO5RVBMCvu9PJLqxM+F7ODEmIZkkSvnBpP25L5ZyXllfdXhFvVlTtGRHgrJCEaLYk4QuXNn9jcq3bL/yyj/ZBPkTLMgpCnDbptBUuaeeRPFKOlZCaVwLAlL4RdAz25cM1iYzpEYZSyskRCtH8SMIXLunyN9dQZrEB8ND5Mdx6dncKyyyUVli5bUIPJ0cnRPMkCV+4pMpkDzA6OhQwV7D65+UDnRWSEM2etOELlxTsVz22vl9HmUkrRGOQGr5wOcXlFnKLK/D3cue6s6LwcJd6iRCNQRK+cDmpuaaj9rnLBjBtcCcnRyNEyyFVJ+FyjuSWAtAx2NfJkQjRskgNX7iMI7klPPLtDorLrQB0koQvRKOShC+a1OtLE8gsKOOJi/ud8NjHaxNZts/MpHV3U7SzX6tWCNE4HJbwlVJdgI+BCEAD72itX3HU/kTz8MIv+wBYtCONT24aSa+IgKpJVCv3ZzE6OpTZ46PJLCyTzlohGpkja/gW4F6t9RalVCCwWSn1q9Z6twP3KZpAUnYxvl7uhPp78f7qQ/xpYAc6tPHFZtNc+uYajhwrYWyPUF6eNQQArTX//i2ezIKyqm1kFJRx88ebyCkq55+XD2Bkt1B2p+XzwNTeTIxp56yXJkSL5rCEr7VOA9LsfxcopfYAnQBJ+C5Oa82Xm5L5dusRnprWn57tAvg+LpVFO9J4alp/xr+wFF9Pd7qF+bM7LZ+V8Vl8dOMIlu3PYFtyLlGhfnwXl8qwqBAqLDbO6hHKq7/HA9CjXQBvXD2UzIIyrnlvPVrDvHVJeLiZWv5Z3cOc+dKFaNGapA1fKRUFDAHWN8X+xJmxWG28u+oQiVlFfGFftOyCV1ZisemqMhsScwAoqbCSWWhq7BsTc5i3/jCPfreT9kE+PHvpAK5+dz2Pfrez1vaX3jeBbvZFz3pFBPLtbWP496/7Wb4/k9ySCsB8IQghHMPhjaRKqQDga+AurXV+HY/PVkptUkptyszMdHQ44hQe/nYH/1y8ly83JdMtzJ9f7hrPKPuyBgCzhnehsNRCp2Bfdj91HhsfOYfPbh5JcbmVR77dSbcwf364Ywyjo0O5dEgn3N0Uj1zQhxFRIdx6dveqZF9pcJdgnp8+kMFdgtmTlk94oDcBckFyIRxGaa3rL3WmG1fKE1gI/KK1fqm+8rGxsXrTpk0Oi0ecXEZBKSOf+53rRkcx94IYALw93NFa831cKp9tSOLTm0ayMzUPDzfFwM7BgGn+Wb4/Ew3ERrYlsMblBq02jbtb/ataZhWWMfXllfSKCOCzm0c55PUJ0ZIppTZrrWPrK+fIUToKeA/Y05BkL5pGaYWVfUcLcHdTPP/LPmaPiyYy1I9l+zPRGmYO74K3h3tVeaUUlwzpxCVDzIzXoV3b1tqeUooJvevuZG1IsgcIC/Dm+zvG0MDiQogz5Mjz5zHAn4EdSqk4+30Pa60XOXCfoh53fLaV3/akV91esb+6Ga1LiC8x7QOdEZZMshKiCThylM4qQOpsTpZyrJhNiceYNrgjSiniko8BMLF3OGN7hrNk11HO69eePWn5jO0pFxYRoiWTHrIWbO/RfKa+vBKAVQlZFJVZyCos587JPbn73F4A3DS2mzNDFEI0IUn4Ldjcb3YQ6O1BQZmFBZtTqu6PDpfrwQrRGsnc9RbscHYxFw3uyEtXDKq6b0Lv8KorSAkhWhep4bdQccm55BSVExHow9ieZvbq7RO7c/95MU6OTAjhLJLwW6hLXl8NQLsgb9oF+rBu7mTCArycHJUQwpmkSacFiU8v4Nr3N/Dr7uphl+72UTft2/jI6pNCtHJSw28hcorKmfH2WnKLK9iWnFt1/6Q+svKkEMKQKp+LK7NYufSN1SzekVZ134QXlp6wMNn8jcnkFldw2ZBO5NkXIlty93jCAuQiIkIIQxK+i/s+LpWtSbk89M0OwKxPk5hdzCfrDtcqt2T3Ufp1DOLB82NwU6AURIb6OSNkIYSLkoTvwr7ZksJce6IvKbdy9/w4Rjz7W9Xjccm52GyaNQey2JqUy2VDOxMR5MOkmAi6hwfUWhNHCCEculrm6ZLVMquVVlgZ/sxv9OkQxIPn9+aWT7ZQWmGlsMxSq1yfDkH4e7mTcqyEZfdPwMfTnbySCorLLXRoI+vTCNEaOH21TFG/zIIyMgpKKSi18Mm6w3QK9mXu+TEopVh7MJuCMgu3T+rBsMgQNj4yGa3hH4v38N+Vh1g4ZyyPfLezqoP26Uv64+NpavRtfD1p4+t5ql0LIVohSfhOkp5v1p8/3sDObbhwYEd2p5prxQztatadV0qhFDx8QR9mjehK9/AAXrtyCOOeXwrAZfbli4UQ4mSkDb8RFZdbuHt+HOsOZtdbdmV8Vq3bt5wdTUz7QP7v570cyirihV/2AdS6oAiYxN893FwGsEuIH29dM5QXZwzCX64UJYSohyT8PyAxq4g9afn8tjsdi9XGy7/F8+3WI8x6Zx1peSVV5X7fk857qw7Veu7K+Op16Kf0jeChqTHMmdST5JwS3liaAJhLANZnav8OTB/WuZFekRCiJZNqYQ1a63rXgz+SW8K+o/l8veUIP21Pq7OMv5c7099cy9DItkyKCefu+dsAGBUdQr+ObbDZNKsTqmv4If5eKKWY3KcdHdr48JV9ZctP/zKykV6ZEKLJaQ15KVBeCHsXQnAUxFwA2Qmw4kXIOQQdB0PMhdBhEBw7BJFnOTQkSfiYETHP/7yPhdtT6RDsS2SIH5cM6UiovzdFZRb8vT34bH0SW5OPsT+9sOp5nYJ9CfTxoEMbH5buMzV2fy933rt+OC/+so8ft6Xy47bUqvIvLdnP238exr70ArIKy7n17O68tfwAk2LMbFgfT3demD6Ia95bT6i/l1zQWwhXVFECljJw84CE36DTUPAOhLRtcHiN+Z2xB8qLoDir7m24eUJod9j6ifmpdP0iiBoDR3fAb0/CpEeg45BTx2Mpb3DorXpY5tG8UhbtSOPDNYkk5RQD0KGND2UWGzlFJ76JI7qFcE6fdtg0jOsZRr+Obaoe+3nnUV5bGs99U3pXXeM1KbuYqa+soLjcyp9HRfLJusOM6RHKuJ7h/HPxXtbNnUxbf88Txsu/vjSB/NIK5p7fx4GvXgjRIFrDvsVw4H+QlwyH10JZHih30NbjCisIj4G2USbZR080iT0kGgrSIGs/+ARDj8nQthtsfBfWvAqRY2Db5+Z55z4NX/7ZbM43BMJ7Q49zYMyd4O4Jiavgf8+YcpYS+OYW1H17GzQss9Ul/LySChIyCtidms+byw6QmlcKwMc3jmB8r3DA1PgX70yjuNxKZIg/6fmlRIX5Myyy7ak2Xadvt6bwzZYjfHzjCD5ak8gTP+4mOsyftLxSdj91nlxSUIjTpbWZSg5grYCKYpN8U7dCu77g3wjXe0iNg62fQvwv5nZuEngFQttICOsJPm1M4m4/wDTReAVAm07QeTi0OcM+tfjfYN7l5m//djD8Jtj1HXh4Q5r9suB9p8GeH0Hbqp8XEo26M04SPpgZqtPfWoNNwyuzBjPl3yuqHuvYxoe+HdswuU87rhzRtVH3W5ficgsDnliC1abpGuLHigcmOnyfQjQLNhuU5pqmEp8gyD4AG96BsgLw9DUJNry3aS7ZsQBsVvALNY+X5VVvxy8MrvwCvPxNk0t4r5Pv01JuviTi5pl9e/iY5x9ebU+wCrqOAv8w6HEuDL4a3B3YzKo1LLgRktbBTUsguEv1Y9/+FbZ9BgHtwT8cxt4FZfmQ8DtMeAjVYaBzE75S6n3gQiBDa92/Ic9xRMLflJjD9LfWnnD/t7edxeAuwU1ew77k9dXEJecyLLItX//VsR00QjSKmjXqP6q82LR75x+BwnTw9IOUjZC4GiqKapf1CoSgDlBRCnlJ5j4PX+hzEQS0g+IcsJRCYAfw8DJNKb88DMU1hkVPfMTsZ/uXcM4TMGCGaUbZ/qWpmdsqwNPf7Cc7wXxJtOsLQ/4MA2eA7+mf1f8hWoPNYppuarJZzdmMp0+dT3OFmbYfAq8BHztwH6e0JekYn29IBsDL3Y2+HYPIL6lg/i2jCQ90ziqSV43sSlxyLgkZhfUXFqIp2WywfzHkJkPqFtj7k2meyD5gRpEMvdYkQrc6RnPnHITNH0LSevAOgJRN4BsMXUebWnl+KnQZAZn7ILf2wn94BcLgK017dXG2qdn6tIE+F5vaNZjRLnkppgPT4xSf3S4jTQ3dWgG/PgZLn61+7Kd7zA+YtvKz7oCI/tBzijmrSN9t9u0d+EfexT9GqROTPYCbu/n5o5t3ZJOOUioKWNjQGn6fqPb6tTsvYPLd71ffWZAORZnmgOo55aTfcMeLS87lsjdWY9PQNcSP5fdPOHlt/vAa2LcICjNMjaNtpGkrC+pketPBfBCCI80pp7sHtB9oDt6wXrU/AEd3mA4ar7ovFF5aYSXm0Z+5bnQkT05r0NsiWiutTVODb1uT7II61a5pa22G/J0sQSVvgPRdpv05YzeE9jC1XZsVRt5iOv+Ks00iD4mG7/4KO7+ufn5gR2jf3zSdbPvc3BcQAb2mwvC/QIeBZmjhT/fCgRqzxttGQdQ4OLjcNLd0GgbuXpAVbxJ5j3OgXR/oe4l5fd5BjmkqKS82TTOhPU07/+HV5ssrvDf0u7TuxNpMNbSG7/SEr5SaDcwGGNbBbdim2QEcC+iB1SuIYFsuHrkHq8oWBUXj064H1qCueF34/Cm/8Z76cTefrjvMojvH0aX8AN6enhDRF4qyzUHv5W8+EN/eYnra3TzMB8jd05wmegWY2/5hENQRko5rFvLwMeUA2nSBs+aAcoNF95kD/rqF5nTRL9Qc5DWUlFvx8nDD3U06bMVxbDZIXg87F5hmjsw91Y8FR8Kov0JpnmnftlpMQm3TxdRsB8wALz84sNRUPBJ+Nc9TbmZESO5h8+VRVD3pD3dvQJtt5ByA8Q/AyFvNZ8u3xsQ/qwX2fA/b5ld3ZLYfCEe3myaRMXfC0D+bIYttoxqlNioartkk/JqioiL1368ZTaTKYKLbViy487plGuV4coX7Mkrwpr9bIgDJvjHkd59Gl3HXEBTRlaIyC8nHiolpHwTA1f/+gVvKP2J8eFF1sh4wAw6thMKjtXfc52K45A1Tu1du5mzip3sgY6/5AOUcNB+Ycfea09XlL5gPw5HNpubSprOpQVW98BrDtToPhyHXmFPb8N5n/maKlilpPfwwxyTKsjyTzCv5h0PvC+BYoqkJ22yQbpbLpuMQiOgH3m1Me3jSWlORAVN5adsN+lwIsTdBYHtTkalsi88+AIkrzZDB3MPw/R0Q3BUGzTLH6qnYrPDb47B/iak0+YXAmLug2ziHvD2iYZplwo+NjdXrN2wkMbuI177+jQo3L26/aCzF5VYOZBbi5e7Gd8vXMzlrHmPdd9JNHSVdhZM363s+Xfgr/jl7GDXjbkb268nKp89lknsc7uExpiael1zdIXL+/5me/4KjEHtjdTvhyWTFmw/Q8aed1gp7B4uXmTm37DnTFDThYVj5omkO2vZZdfnHcqTmI0wb+Y6vzBnmgd/N8dPnYshPMcfUkD9Dv0tObBa0WmD3d2aiT0h07cdKjsFP95mZm8NvbnDTp2gZmm3Cb+gonQqrjc0L32HU1gdPeOxgwDCiCzezv+/f6HXF09UPWMpMbej4D0tjyYo37axe9itNaQ3vTjZnAgA3LHb41GnRyGw2M6pE20wCDuoIVvukvLDepsZckGbGTbt7mJEjWfGQHW+aT2L+VHt7R3fAu+eYSkhoD1PxmPQY9JrS9K9NtBhOT/hKqc+BCUAYkA48rrV+71TPOZNhmbtWfEt5TjKh/p6U7f8fHunbOEYg5Z5B9LjlM8LadTjTl9A4KkpM7/+86eZMYvoHpiMMIHM/hHQ7eeeR1QJFGfDjneZMpPf5TRd35dmLZwu6iErKJtj2BWTuhSNbTPPFiNkQ1sN0bmbtN0PyshNg+3wz5vnwmupmlOOF9TKJ/vAqM4ql7zRY+g/Td1MpsIP56TvNDD/ct9h0ss5e6riKh2h1nJ7wz0RjjMMvLLPg5e6Gl4eLLQR6aIWZVFGaD7ethaIseH8K9DrfdHj99riZXn3O46a8tQLeO9dMDKk0YrZJElFjzQiKumYU1jdm+mTjfAH2LISfHzLtyGUFgDajGQZdCd3ONiMd/EL+0NvgcAVHTc3aw9vMUkxaZ5o3dn5z4nBANw/zXoTHmC+BuoR0hxE3m236h5kRL55+5ot884fmi6LvxWZESnGWmXk56TEzKebQcnNWuWehGWPu3860k8feaL7ohWgkrTbhu7RjifDKIDNMzCeouqmnput/MiMuPrvCrN0x9h6TSLYcN50hqBPcU6Oj2GaFL6+F+CUw4SEz9O3QStMckXPQ1EBj/gTxv5oRIO5e5r7Ow01S8g40a3pE9DfNTm4eZijgwaXVHYkevuYLyTcEek81o48KM0znYuWXTMFR04HtH26Sn6MdWArr3zbNKYUZZoSLf7hpginJre48D+oE4+6BgTNNsgbz+62x5ktw6J+hcyzk2Zv82g8ww257TD55v4vW5n139zBt6GnbzVjz48+KinPMF0X7AdKHIxxCEr6ren1kdW2y9wVm/L+bJ8zZDG+PM804I26G1S+bBHHLSpNMM/aYRPT22dVD9TqPMAlmyjNmgsmKFyC8T+2hfP7hJgHlJlXfp9zMnIb8VNOm7BNkknqnYXDdj7U7Cy3l8PuTsPZ1c1ZQ2X7dYbCprf78EPS7zMxojF8Cvz9lFnRy94Lx95vX2P4PzDew2SA3ETZ/ZGKOGgO7v4eV/zJrmRxabkaqePlDQap5/ccSTQwjZpvhsrlJpqYeVEfzXskxM/HHkVPmhXAwSfiuqqIUno0wTTJ/22oS+/CbTfNM5n5Tsz92yHwJzE05cbTFweXw6WWmTbhykkz/6abWPvhqMwLppX5miJ9XANz0q2my2PC2qZnnJZtx1t7mqllYLSbZFWWbZou6ZlFqbZ9w5gWb3zdnCQm/126rrhQxACY8aIb6lZrr7dJxiJncM3CW2YZS9dd0E343zVmZ+2DHl9X397vMrC0e2N40O5Ucg4teNSNb0neaM5SSHPuszMEN+58I0cxJwndl+akm+dY1dK6swHT8eQfAxIdPvZ2seHjN/j8e/hc4/wWTsEuOmb6CtpGNH3ul8iLTGR0SbabhF2aYsdxdRpp1TcqLzASffT+b5qiMXdCmq1kTxc3TzImY+HDtBaLAvDcePvDvfqbPAMwXV88pZuTL0R3mC+TKL8yXR1ocdJ/kuNcpRDMgCb+1+PFO04Rxzbd1185dgdamL2D589WT4Dx8TU2/41CY8pRpIvrxztoXgxh0lZlcNPp2U9ZmMyNdOg4xXypCCEASfuvSmKsZOlpJrpmlfOwwLH4A9v9s7u9xjulgHnyNGevu08YsbiWEqJcrrJYpmkpzSfZQvT5L20i4aj4cXAZrXjMjkjoOgWmvNa/XI0QzIglfOFf0BPNTkG6aaSTZC+EwkvCFawiMcHYEQrR4LtrLJ4QQorFJwhdCiFZCEr4QQrQSkvCFEKKVkIQvhBCthCR8IYRoJVxqpq1SKhM4XG/B2sKALAeE09gkzsbVHOJsDjGCxNnYnBFnpNY6vL5CLpXwz4RSalNDphQ7m8TZuJpDnM0hRpA4G5srxylNOkII0UpIwhdCiFaiJST8d5wdQANJnI2rOcTZHGIEibOxuWyczb4NXwghRMO0hBq+EEKIBpCE34iUkrV9G4u8l41L3s/G1Vzfz2aR8JVSE5VSsn5uI1JKufr/PgBAKVXP1c6dSyl1sVKqu7PjaICq99HVk1UzODahmRyfx3PpN1YpdY5SaiPwFS4cq1JqmlLqG+AZpVS0s+M5GXtyusfZcZyMMtoppZYB7wJora3Ojapu9mNzLfAe0MHZ8ZyMUupPSqnfgJeUUuMBtAt23Ln6sQnN6/g8GZdMokqpNkqp74FHgAeAQ0Cs/TGXqp0opfoBzwAfAP7AfUqpafbHXOL9VUp5KKUeBF4FXlRKDdZa21ytdmJPRKX2n4FKqfPBpd5HpZQKUEr9CPzd/rMOiLQ/7hJxVlJKRQHPAv8B9gCzlVJ/sT/mErE2l2MTXP/4bAhXDdQd+EprPVFrvRT4ARgJLlk7GQ78qrX+EfPhWgHcqJQK0lrbnBuaobW2APuAGOAe4G37/S5VO7F/cDoDccBDwGMALvQ+aq11IfCp1nqC1vp34Bdgmv1xl4izhu7AKq3195gKybvAHKVUW3tSdXrlqbkcm+D6x2dDuEzCV0rNUUo9qJSarLXO0Vp/ar9fARqw2G87NWal1PlKqT417toJTFVKeWutM4HlQDIw2ykB2iml/qaU+qdS6gr7XT9prUu11i8D7ZRSV9nLebpAjJdD1QcnFegFrAbSlFK3KqV6OivG4+KcYY9zvv1+N+AYkKyU8nZmjPZ4piulRta4KwW43H5slmqtlwFrsCcqZ2kOx6Z9/83i+DwtWmun/gCemCaRFcANQBIwFfCpUeZ8IN7JcXYE1gJHgJeonsPgjqk5PWi/7Q1cZC8T4IQ4FXA35oCcjjmVvx5oV6PMpcARJ76XJ4sxBNN097i93H1AEfCj/baHi8QZXqPMWcBeJx+b7TAVjVTgO8CtxmMfAy/XeD2DgAVAhAv9313m2GxOx+eZ/LhCDd+CaRaZo7X+ANMuOsN+X6WNwDal1CgnxFepDJgP3IQ5IC6p8dg84DylVE+tdRlQDoRpc/rfpLQ58iYCf9daL8AcuAOB82qU+RbYr5S6D0wHpAvEOBg4FzgKjFNKLcJUAFYDB+1PbdLT/JPEOQhTIaksswZIUUpd3JSx1aS1zgC+t8eVBtxS4+EngQuVUv3sr6cUKADk2Dy9OF3u+DwTzm4eUfY3dxvmg4TW+mMgExijlAqzF/XGJNl8pwRq4srGdCytwfyDJyulQrVpa1yJqWG9o5TqCIwHrEopj6aMsUZz1yZgnD3un4F4oJ9SqneN4n8FnldKHQU6uUCM+zDHwBBMU8RGrXU/YBYwQSnVyX6sODvO/Zj3MsZeLgjYC1Q0VWwnifM/wG5gCfAnpVQHAK31AcxIojeUUmOBazBnBE3a7twcjs164nSp4/NMNWnCP75NrsYblAd0VEp1tt/+FtNJ62EvdwRoj/1LoanjrKS1tmmt8zHf6hWYfzZaa4vW+klgKfAKptbyoDYdUk0Rr1tlfPa7EoBApdQA++3lQBsg0F5+MPBf4GtgqNb6IxeIcYU9vgzgVq314/byOcAY+zHgcKfxXgbYy+VjOvKadJ7I8XFqrSvsx9sazBfQnZVltdb/wCT9m4DewE1a6xIHx+du/61qxomLHZunEadLHJ9/VJMkfKXUKKXUF8ALSqn+Ne6vrAH/ihlRMFwp5aG1Xo9pL5tUYzNTtNafOylOddyIhl3AViBGKRWklGoHoLV+CrhOa32R/TTbUXGOVkq9qpS63r5fm/3+yvdzA6apbIr9/dyNqSlVrtGdDdymtZ6htU51kRh3YYY3DtFalyql3Gt8CB3W/NAI7yXALK31h46KsZ443Y47NrMwo9p6KaU6KzNuvK39zPkWrfUVWuujDoxzjFLqI+DvSqmQykpdjUqU04/NM4zTKcdnY3N4wrePbHgTWAj4YIZeoZRyr6wBa603ANuBCcC19qdmYmoq2MsUOTFOrbXWSilPpZSbvZ3+S8wBsRNYquwzgbXWxQ6OczrwGqZfY7JS6hll5gJQ4/1MwJySdscMHwPTB3HY/niy1nqHi8aYaH/c6uhT5MaI016m1Ilx2uzHprcyo3GsWusVmErJTkwNOsxettzBcUYDb2DOdCOBp5VSF9j3XWH/7dRjsxHiTLQ/7vDj0xGaoobfC9OL/SnwMphvUXvbN/aD9yXgG0xTziyl1HZMwo9rgvgaGueTmHbSYHv5OzGnyJ9hvvXTmyjO/sA3WutPMJPSRgJXKKUqp3o/o5R6D9iM6XMYoXdcBlcAAAaUSURBVJTaDORgxoy7eoxLmijGlhTnk5iRYh3st2/FdNy+DQzUWsc3UZyxwB772c59mM/vRcren+Aix+YfjbMp/++NrtE7FZUZW9sZWKu1XouppT+plCrEfFNuAF5XSr0PdAGiMcOcjgBHlFK7MM37Dr0m5BnE2RN4zN5mB6bzboTWeu+JW3donNmYscrBWus0pVQ6ppZytlKqCPN+Pqa1TrQ//yrMcLHc1hxjC4+zJ/BoZZyY9uez7LVUR8Y5CsjRWu+337UB+JtSqqvWOkkptRpTQ56lzBIpzno/m0WcTaHR1sNXpvPjEcysw3nAjcDDwI+Ytvi7gbe01guVUs9hhjB9XFn7sDedOHxYUyPE6aGboDP2JHHeC+Ta/w7BTEizYhLCBns7beXz3bSDZwA2hxhbWZxN9RkKtsc3Hvg/zDj/QqVUe+AuIFNr/S/767kKk0Bf1lrn2Z/fVO9ns4izSenGnbAwD5hg//tyTBtZb/vtBUAv+98jgcXYJyYB7o0ZRwuNczqmc7sv5oM/E7jR/tg1wJc1nucmMUqcDoyxE/A3zGSkfwAX2O93x8xP+RAYab9vEvC7xOkaP3+oDV8pda1S6mz7NylAOtDWXgv+GtNxdKX9G/UAJrmCmcRQghlbj3ZwraSFxLkA04w0S5ulJ+Zrrd+3l+uN6f/AHqfDaiXNIUaJ06FxBmnT/PoOZuBCKaaNu5P987EWM4LtJXsfQz/gsFLKT+J0vtNO+MrooJRaClwHXI1p6w7ADAkbgH2MMmZkwSWYU9AlmGGX6zAzaR/WWhc0wmtoTXG+Ckyr0bk02d7nMQRY1ZpjlDibLM43lVJh2qx9Uwz8BrTFPoRaa52utX4FWA+8jzkT+T/twNFrzSVOl3Cap0ju9t+9MCsGgjk9ehPzpgUDP2PazPzsj3+FGVcL5iAe4OjTlhYe53zgTvvf3YFLW3uMEmeTxvkfzIihmmXvxqyH1QYIrFE2UOJ0rZ8GjdKxd2o8Df/f3t2D2FGGURz/nwQVi4gWgoVfaKVEVAQjQhIVrETcRhBkJSCKiGKjlQQSDKQQbYIiWLidJgqiLIiKhRILkd2QQhBEUwT8QsEYCYhmj8UzcRcN7r3mfszd9/zgFjszdzm7LM/dmXnnedis6iFxEV3fCNunJT1B9fB4gVqm+AC1ROwg1VdmqTv2N2Cc679byPkn1YMd12PzX7eaMTmnkvMp4FtJO21/3L3tVaqQfghcJelm10NT4zwznomcfbPuJR1JO6lCeAm13Os5qq3AnZJuhb+vbe8FnnetGvgAeEjSEWrp51gfpEjO9jIm59RyrgB7utcZ9wCPUz2xbvAYn5CdpZy9NMAp03Zgfs3XL1PNjXYBS922TVSvm7eAK7ptlwHXTOpUJTnbypicU895CLi623YfsCM5+/8a5KbtEnBIqyPHPgWudD2ltlnSk65P08uBP2wfB7D9ve1vzvodxyM528qYnNPNedrdg0m233G1c0jOnlu34Ns+Zft3ry5JvJtqewDVD/o6SYvA68DyeGKuLznbygjJOWr/J6c0+TGJs5KzjwZurdB9mppqA/tut/kk9ZTqVuCYe9AiNDlHZxYyQnKO2jA57bpWMg2zkrNPhlmHv0KNI/yJmti+COwGVmwf7sMfaic5R2cWMkJyjlpyblTDXPAHbqN+yYepIQpTvwmRnMmYnMnZ95x9eQ3VPE01kWoeeNHVE76XknN0ZiEjJOeoJefGNLJumRER0W9THWIeERGTk4IfEdGIFPyIiEak4EdENCIFPyKiESn4ER1JeyQ9/R/75yRdP8lMEaOUgh8xuDlqvmzETMo6/GiapGepsXg/AsepTowngEeB86l+6/PUfOPFbt8JVucevwRcCpwCHrH95STzRwwjBT+aJekWYAHYRjUSXAZeAV6z/XN3zD7gB9sHJC0Ai67h4kj6CHjM9leStgH7bd81+Z8kYjADd8uM2IC2A2+7G1wt6UzHxa1dob+Ymm/8/j/fqBo4fjvw5prOuxeMPXHEOUjBj/i3BWDO9lFJu4A7znLMJuAX2zdNMFfEOclN22jZJ8CcpAslbQHu7bZvAb6TdB7w4JrjT3b7sP0rcEzS/VADNiTdOLnoEcNLwY9m2V4GDlJDrd8DPu927QY+o0bnrb0J+wbwjKQjkq6lPgwelnQU+IKamRrRW7lpGxHRiPyHHxHRiBT8iIhGpOBHRDQiBT8iohEp+BERjUjBj4hoRAp+REQjUvAjIhrxF508wKqnuxIKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the RL estimator\n",
    "estimator = RLEstimator(role=role,\n",
    "                        source_dir='src/',\n",
    "                        dependencies=[\"common/sagemaker_rl\"],\n",
    "                        toolkit=RLToolkit.COACH,\n",
    "                        toolkit_version='0.11.0',\n",
    "                        framework=RLFramework.MXNET,\n",
    "                        entry_point=\"evaluate-coach.py\",\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type=instance_type,\n",
    "                        hyperparameters = {\"evaluate_steps\": 253*2})\n",
    "\n",
    "# Perform the evaluation\n",
    "estimator.fit({'checkpoint': s3_checkpoint_path})\n",
    "\n",
    "# Get the data from S3\n",
    "job_name=estimator._current_job_name\n",
    "output_tar_key = \"{}/output/output.tar.gz\".format(job_name)\n",
    "intermediate_folder_key = \"{}/output/intermediate/\".format(job_name)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "\n",
    "wait_for_s3_object(s3_bucket, output_tar_key, tmp_dir)  \n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "os.system(\"aws s3 cp --recursive {} {}\".format(intermediate_url, tmp_dir))\n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "print(\"\\nCopied output files to {}\\n\".format(tmp_dir))\n",
    "\n",
    "# Plot the RL Policy performance against a buy and hold policy\n",
    "df_info = pd.read_csv(tmp_dir + '/portfolio-management.csv')\n",
    "df_info['date'] = pd.to_datetime(df_info['date'], format='%Y-%m-%d')\n",
    "df_info.set_index('date', inplace=True)\n",
    "df_info[[\"portfolio_value\", \"market_value\"]].plot(rot=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
