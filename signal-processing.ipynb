{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Processing (LSTM Training)\n",
    "\n",
    "Now that we have the data processed properly the challenge will be to design, train and test the Long Short-Term Memory (LSTM) network to predict the classifications we have previously extracted.\n",
    "\n",
    "For reference here is the image again of the full Deep Deterministic Policy Gradient (DDPG) Reinforcement Learning (RL) architecture we are trying to build.  Please see the full [2nd report](docs/report2.pdf) for a complete description of this network.\n",
    "\n",
    "![DDPG](docs/ddpg.png \"DDGP\")\n",
    "\n",
    "As you can see there is a LSTM in both the actor and critic networks.  Before the full DDPG can be implemented we must confirm that the LSTM can provide satisfactory signals or the DDPG will just be running on noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load some necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#import random\n",
    "#import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import time\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and encode the timeseries data \n",
    "Each data _point_ at time `t` is actually a series of `k` standardized return values from `t-k` to `t` (`x`) and an array of classifications for each `dt` in `prediction_days` (`y`).  Therefore each data point is treated as independent events and will be processed in a random order.  So our first task is to properly structure the timeseries data into indepentent events.\n",
    "\n",
    "Here is an illustration of this transformation for a single asset with two different number of time values.\n",
    "\n",
    "![embedding](docs/embedding.png \"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the previously saved data and unpack it into Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training-data-raw.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "x_raw = data['x'].values\n",
    "names = data['x'].columns.tolist()\n",
    "dates = data['x'].index.tolist()\n",
    "dt_values = list(data['y'].keys())\n",
    "\n",
    "y_raw = np.empty((len(dates), len(names), len(dt_values)))\n",
    "for i, dt in enumerate(dt_values):\n",
    "    y_raw[:, :, i] = data['y'][dt].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the settings file as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('settings.yml') as f:\n",
    "    settings = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Process the input data `x`\n",
    "Both `x` and `y` are transformed into 3D matrices as shown below.  Note that the 1st 2 dimensions are the same but the 3rd dimension is different.\n",
    "![shape](docs/data-shape.png \"shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = settings['embedding_days']\n",
    "n = x_raw.shape[0] - 1\n",
    "x = np.empty((n - k + 2, len(names), k))\n",
    "for k_i in range(k):\n",
    "    x[:, :, k_i] = x_raw[(k - k_i - 1):(n + 1 - k_i), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check on the data.  Should be all 0.0's; ie. max(abs(delta)) = 0.0.\n",
    "Note how the embedded values are in reverse order (latest first) of the raw data (oldest first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_delta = 0.0\n",
    "for t in range(x.shape[0]):\n",
    "    max_delta = max(max_delta, np.abs(x_raw[t:(t+k), :] - x[t, :, ::-1].T).max())\n",
    "max_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Process the target values `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2928, 70, 63)\n",
      "(2928, 70, 3)\n"
     ]
    }
   ],
   "source": [
    "y = y_raw[(k - 1):, :, :]\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data into training, validation and testing sets\n",
    "The `test` set is not used during the training and is only used at the very end to evaluate how well the LSTM can predict unseen data.  For our purposes we defined the `test` set as data from the period from the `training_end` date defined in the settings file to the end of our processed data.\n",
    "\n",
    "The `train` set is the data actually used to traing the LSTM where as the `val` set isn't directly used to traing the LSTM but is used to evaluate the training process after each training epoch.  After each epoch we evaluate the model against the `train` and `val` set.  We will continue to train as long as both the `train` and `val` errors decay, but we must stop if the `val` error begins to rise.  A falling `train` error but rising `val` error indicates the model is starting to overfit the training data.  A good description of overfitting is contained in this [wiki page](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "We will follow the stardard practice of splitting our non-test data 9-1 between the `train` and `val` sets.  Note also that we are randomizing the data within the `train` and `val` sets to emilinate any temporal biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_days = data['x'].loc[settings['training_end']:, :].shape[0]\n",
    "    \n",
    "# Test data is simply all the data after the 'training_end' date.\n",
    "x_test = x[-n_test_days:, :, :]\n",
    "y_test = y[-n_test_days:, :, :]\n",
    "\n",
    "# Extract the train and val data and then randomly split 9/1\n",
    "x_train, x_val, y_train, y_val = train_test_split(x[:-n_test_days, :, :], \n",
    "                                                  y[:-n_test_days, :, :], \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is now ready, let the real fun begin...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the LSTM network\n",
    "Please see the PyTorch LSTM [documentation](https://pytorch.org/docs/stable/nn.html#lstm) for more detail.    \n",
    "\n",
    "A special thanks for this component goes to the Udacity Deep Learning [Nanodegree](https://www.udacity.com/course/deep-learning-nanodegree--nd101).  Much of this content was derived from the Nanodegree projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Check to see if running on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; ouch, consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; ouch, consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_output, n_dt, k, n_layers=1, n_hidden=500, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch LSTM Module plus a linear layer to perform the regression classification.\n",
    "        \n",
    "        Args:\n",
    "            n_features (int): The number of input dimensions (1 if only 1 asset modeled in isolation)\n",
    "            n_dt (int): Number prediction horizons\n",
    "            n_output (int): The number assets we are predicting\n",
    "            k (int): The time embedding \n",
    "            n_layers (int): Number of LSTM layers\n",
    "            n_hidden (int): Number of hidden nodes in the LSTM layers\n",
    "            dropout (float): dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Set class attributes\n",
    "        self.n_features = n_features\n",
    "        self.n_dt = n_dt\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_size = n_dt * n_output\n",
    "        \n",
    "        # Define the LSTM\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden, num_layers=n_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Linear fully-connected feed forward network\n",
    "        self.fc = nn.Linear(hidden_dim, self.output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): [batch_size, k, n_feature] The input to the neural network\n",
    "            hidden (tuple of tensor): The previous hidden state\n",
    "        \n",
    "        Returns:\n",
    "            tensor:  [self.output_size] Ouput of the network\n",
    "            tuple of tensor:  (h_n, c_n) The latest hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # Get LSTM outpout and updated hidden state\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "    \n",
    "        # Stack up lstm outputs\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "\n",
    "        # Feed through fully-connected layer and apply the signmoid function to limit to [0,1]\n",
    "        out = self.sig(self.fc(out))\n",
    "        \n",
    "        # Reshape to be batch_size first\n",
    "        out = out.view(x.shape[0], -1, self.output_size)\n",
    "        out = out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        \n",
    "        Args:\n",
    "            batch_size: The batch_size of the hidden state\n",
    "        \n",
    "        Returns:\n",
    "            tuple of int:  hidden state of dims (n_layers, batch_size, n_hidden)\n",
    "        '''\n",
    "        # Initialize hidden state with zero weights, and move to GPU if available\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the forward and backpropagation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(lstm, optimizer, criterion, x, y, hidden, clip=5.0):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    \n",
    "    Args:\n",
    "        lstm (LSTM): The LSTM object\n",
    "        optimizer: The PyTorch optimizer for the neural network\n",
    "        criterion: The PyTorch loss function\n",
    "        x (tensor): [batch_size, k, n_feature] The input to the neural network\n",
    "        y (tensor): [batch_size, n_dt * n_output] The neural network output\n",
    "        hidden (tuple of tensor): The previous hidden state\n",
    "        clip (float):  Value to clip gradients to avoid exploding LSTM gradients\n",
    "    \n",
    "    Returns:\n",
    "        float: The loss for the last training batch item\n",
    "        tuple of tensor:  (h_n, c_n) The latest hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        x, y = x.cuda(), y.cuda()\n",
    " \n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # Zero accumulated gradients\n",
    "    lstm.zero_grad()\n",
    "    \n",
    "    # Perform forward propagations, loss calculation and back propagation\n",
    "    output, hidden = rnn(x, hidden)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the gradients and then perform the weight optimization\n",
    "    nn.utils.clip_grad_norm_(lstm.parameters(), clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
