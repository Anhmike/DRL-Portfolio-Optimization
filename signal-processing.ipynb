{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Processing (LSTM Training)\n",
    "\n",
    "Now that we have the data processed properly the challenge will be to design, train and test the Long Short-Term Memory (LSTM) network to predict the classifications we have previously extracted.\n",
    "\n",
    "For reference here is the image again of the full Deep Deterministic Policy Gradient (DDPG) Reinforcement Learning (RL) architecture we are trying to build.  Please see the full [2nd report](docs/report2.pdf) for a complete description of this network.\n",
    "\n",
    "![DDPG](docs/ddpg.png \"DDGP\")\n",
    "\n",
    "As you can see there is a LSTM in both the actor and critic networks.  Before the full DDPG can be implemented we must confirm that the LSTM can provide satisfactory signals or the DDPG will just be running on noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load some necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and encode the timeseries data \n",
    "Each data _point_ at time `t` is actually a series of `k` standardized return values from `t-k` to `t` (`x`) and an array of classifications for each `dt` in `prediction_days` (`y`).  Therefore each data point is treated as independent events and will be processed in a random order.  So our first task is to properly structure the timeseries data into indepentent events.\n",
    "\n",
    "Here is an illustration of this transformation for a single asset with two different number of time values.\n",
    "\n",
    "![embedding](docs/embedding.png \"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the previously saved data and unpack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training-data-raw.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "x_raw = data['x']\n",
    "y_raw = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the settings file as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('settings.yml') as f:\n",
    "    settings = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Process the input data `x`\n",
    "For each asset lets create the return matrix with rows for each `t` and columns for time embedding `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = settings['embedding_days']\n",
    "n = x_raw.shape[0] - 1\n",
    "x_all = {}\n",
    "for i, asset in enumerate(x_raw.columns):\n",
    "    x = np.empty((n - k + 2, k))\n",
    "    for k_i in range(k):        \n",
    "        x[:, k_i] = x_raw.iloc[(k - k_i - 1):(n + 1 - k_i), i].values\n",
    "    x_all[asset] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check on the data.  Should be all 0.0's; ie. max(abs(delta)) = 0.0.\n",
    "   Note how the embedded values are in reverse order (latest first) of the raw data (oldest first).\n",
    "   This a brute force check so you can set `run_test = False` to save time if you have faith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test = True\n",
    "if run_test:\n",
    "    max_delta = 0.0\n",
    "    for i, asset in enumerate(x_raw.columns):\n",
    "        for t in range(x_all[asset].shape[0]):\n",
    "            max_delta = max(max_delta, np.abs(x_raw.iloc[t:(t+k), i].values - x_all[asset][t, ::-1]).max())\n",
    "    max_delta        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Process the target values `y`\n",
    "For each time in `x` we need the classification for each of the prediction lengths `dt`.  Therefore `y` will be a matrix with `len(dt)` columns and `(n-k+2)` rows like `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = {}\n",
    "for i_asset, asset in enumerate(x_raw.columns):\n",
    "    y = np.empty((n-k+2, len(settings['prediction_days'])))\n",
    "    for t in range(x_all[asset].shape[0]):\n",
    "        for i_dt, dt in enumerate(settings['prediction_days']):\n",
    "            y[t, i_dt] = y_raw[dt].iloc[t+k-1, i_asset]\n",
    "    y_all[asset] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data into training, validation and testing sets\n",
    "The `test` set is not used during the training and is only used at the very end to evaluate how well the LSTM can predict unseen data.  For our purposes we defined the `test` set as data from the period from the `training_end` date defined in the settings file to the end of our processed data.\n",
    "\n",
    "The `train` set is the data actually used to traing the LSTM where as the `val` set isn't directly used to traing the LSTM but is used to evaluate the training process after each training epoch.  After each epoch we evaluate the model against the `train` and `val` set.  We will continue to train as long as both the `train` and `val` errors decay, but we must stop if the `val` error begins to rise.  A falling `train` error but rising `val` error indicates the model is starting to overfit the training data.  A good description of overfitting is contained in this [wiki page](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "We will follow the stardard practice of splitting our non-test data 9-1 between the `train` and `val` sets.  Note also that we are randomizing the data within the `train` and `val` sets to emilinate any temporal biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
